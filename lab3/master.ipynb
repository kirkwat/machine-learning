{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Three: Extending Logistic Regression\n",
    "\n",
    "Team: Miro Ronac, Kirk Watson, Brandon Vincitore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation and Overview\n",
    "\n",
    "### 1.1 Task and Use-case\n",
    "\n",
    "This data can be useful in identifying prediabetes or diabetes in patients and assisting doctors with making accurate observations from a variety of health indicators.\n",
    "\n",
    "Every year, the CDC collects data from a health-related telephone survey called the Behavioral Risk Factor Surveillance System. The data gathered from these surveys include information on “health-related risk behaviors, chronic health conditions, and use of preventive services.” This dataset focuses on responses from 2015 and diabetes, a “prevalent chronic disease in the United States.”\n",
    "\n",
    "Ultimately, the ability to identify a patient with prediabetes or diabetes with increased efficiency and accuracy is the intention of analyzing this dataset. With this capability, a diabetes diagnosis can be reached at a faster rate compared to when a human makes the diagnosis. Diabetes is extremely common in the US as about 1 in 10 Americans have diabetes, and [about 1 in 5 people with diabetes don’t know they have it](https://www.cdc.gov/diabetes/library/spotlights/diabetes-facts-stats.html#:~:text=37.3%20million%20Americans%E2%80%94about%201,t%20know%20they%20have%20it.). In addition, 1 in 3 Americans have prediabetes, and [more than 8 in 10 adults with prediabetes don’t know they have it](https://www.cdc.gov/diabetes/library/spotlights/diabetes-facts-stats.html#:~:text=37.3%20million%20Americans%E2%80%94about%201,t%20know%20they%20have%20it.). Using this classifier, patients that might be at risk of diabetes can be brought to a doctor’s attention at a higher rate allowing for earlier medical care and attention.\n",
    "\n",
    "Dataset Source: https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset\n",
    "\n",
    "### 1.2 Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 253680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
       "0           0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
       "1           0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
       "2           0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
       "3           0.0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
       "4           0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
       "\n",
       "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
       "0                   0.0           0.0     0.0  ...            1.0   \n",
       "1                   0.0           1.0     0.0  ...            0.0   \n",
       "2                   0.0           0.0     1.0  ...            1.0   \n",
       "3                   0.0           1.0     1.0  ...            1.0   \n",
       "4                   0.0           1.0     1.0  ...            1.0   \n",
       "\n",
       "   NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  \\\n",
       "0          0.0      5.0      18.0      15.0       1.0  0.0   9.0        4.0   \n",
       "1          1.0      3.0       0.0       0.0       0.0  0.0   7.0        6.0   \n",
       "2          1.0      5.0      30.0      30.0       1.0  0.0   9.0        4.0   \n",
       "3          0.0      2.0       0.0       0.0       0.0  0.0  11.0        3.0   \n",
       "4          0.0      2.0       3.0       0.0       0.0  0.0  11.0        5.0   \n",
       "\n",
       "   Income  \n",
       "0     3.0  \n",
       "1     1.0  \n",
       "2     8.0  \n",
       "3     6.0  \n",
       "4     4.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('diabetes_012_health_indicators_BRFSS2015.csv')\n",
    "print('Size of dataset:', df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 5680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NoDiabetes</th>\n",
       "      <th>PreDiabetes</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NoDiabetes  PreDiabetes  Diabetes  HighBP  HighChol  CholCheck  BMI  \\\n",
       "0           1            0         0       1         1          1   40   \n",
       "1           1            0         0       0         0          0   25   \n",
       "2           1            0         0       1         1          1   28   \n",
       "3           1            0         0       1         0          1   27   \n",
       "4           1            0         0       1         1          1   24   \n",
       "\n",
       "   Smoker  Stroke  HeartDiseaseorAttack  ...  AnyHealthcare  NoDocbcCost  \\\n",
       "0       1       0                     0  ...              1            0   \n",
       "1       1       0                     0  ...              0            1   \n",
       "2       0       0                     0  ...              1            1   \n",
       "3       0       0                     0  ...              1            0   \n",
       "4       0       0                     0  ...              1            0   \n",
       "\n",
       "   GenHlth  MentHlth  PhysHlth  DiffWalk  Sex  Age  Education  Income  \n",
       "0        5        18        15         1    0    9          4       3  \n",
       "1        3         0         0         0    0    7          6       1  \n",
       "2        5        30        30         1    0    9          4       8  \n",
       "3        2         0         0         0    0   11          3       6  \n",
       "4        2         3         0         0    0   11          5       4  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating target features by modifying first column in original dataframe such that we have 3 features consisting of binary values for\n",
    "# no diabetes, prediabetes, and diabetes where 0 is False and 1 is True\n",
    "target_array = np.zeros((len(df),4))\n",
    "for i in range(len(df)):\n",
    "    target_array[i,0] = df['Diabetes_012'].values[i]\n",
    "for i in range(len(target_array)):\n",
    "    # no diabetes\n",
    "    if target_array[i,0] == 0:\n",
    "        target_array[i,1] = 1\n",
    "    # prediabetes\n",
    "    if target_array[i,0] == 1:\n",
    "        target_array[i,2] = 1\n",
    "    # diabetes\n",
    "    if target_array[i,0] == 2:\n",
    "        target_array[i,3] = 1\n",
    "\n",
    "# Adding new target columns to original dataframe\n",
    "target_columns = ['NoDiabetes', 'PreDiabetes', 'Diabetes']\n",
    "for i in range(target_array.shape[1]-1):\n",
    "    df.insert(i, target_columns[i], target_array[:,1:][:,i], True)\n",
    "df_target = df.drop('Diabetes_012', axis=1)\n",
    "\n",
    "columns = list(df_target.columns)\n",
    "for col in columns:\n",
    "    df_target[col] = df_target[col].astype(int)\n",
    "    \n",
    "df_target.drop(df_target.tail(248000).index,inplace = True)\n",
    "print('Size of dataset:', df_target.shape[0])\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataset Training and Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['NoDiabetes', 'PreDiabetes', 'Diabetes']\n",
    "for col in targets:\n",
    "    columns.remove(col)\n",
    "    \n",
    "# Splitting dataset\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "train, test = tts(df_target, test_size=.20, random_state=42, shuffle=True)\n",
    "\n",
    "X_test  = test[columns].to_numpy()\n",
    "X_train = train[columns].to_numpy()\n",
    "y_test = test[targets].to_numpy()\n",
    "y_train = train[targets].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a large dataset of over 250,000 instances, an 80/20 split is sufficient. Our classifier has plenty of data to use for training and also has plenty of data to use for training. We could comformtably move our split to 75/25 or 70/30 if we desired more opportunities to test our classifier. With such a large dataset, we could also divide the dataset with an additional validation set. With a validation set, we can use this set for more frequent model evaulations and save the testing dataset for a final unbiased evaluation. \n",
    "\n",
    "UPDATE: We reduced the size of the dataset to allow for reasonable runtime. With 5680 instances, the dataset is large enough to the above to still be valid. However, we do not have as much freedom to possible create a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling\n",
    "### Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta=0.01, iterations=50, optimization='sd', regularization='none', C=0):\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) # return the actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    # private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # creating ability to choose optimization technique\n",
    "    def _get_gradient(self,X,y):\n",
    "        \n",
    "        gradient = None\n",
    "        if self.opt == 'sd': \n",
    "            gradient = self.steepest_descent\n",
    "        elif self.opt == 'sgd': \n",
    "            gradient = self.stochastic_gradient_descent\n",
    "        elif self.opt == 'newton': \n",
    "            gradient = self.newton\n",
    "        else:\n",
    "            print('No optimization chosen')\n",
    "        return gradient(X,y)\n",
    "    \n",
    "    def steepest_descent(self,X,y):\n",
    "    \n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "    \n",
    "    def stochastic_gradient_descent(self,X,y):\n",
    "        \n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "    \n",
    "    def newton(self,X,y):\n",
    "        \n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        \n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        \n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += self.C * self._get_reg_gradient()\n",
    "        return pinv(hessian) @ gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # regularization methods\n",
    "    def _get_reg_gradient(self):\n",
    "        \n",
    "        if self.reg == 'none':\n",
    "            return self.w_[1:]\n",
    "        elif self.reg == 'L1':\n",
    "            return np.sign(self.w_[1:])\n",
    "        elif self.reg == 'L2':\n",
    "            return -2 * self.w_[1:]\n",
    "        elif self.reg == 'L1_L2':\n",
    "            return -2 * self.w_[1:] + np.sign(self.w_[1:])    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        for _ in range(int(self.iters)):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, eta, iterations, optimization, regularization, C=0):\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.C = C\n",
    "        self.encodings = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        self.classifiers_ = []\n",
    "        for i in range(len(y[0,:])):\n",
    "            blr = BinaryLogisticRegression(self.eta, self.iters, self.opt, self.reg, self.C)\n",
    "            blr.fit(X,y[:,i])\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # saving weights\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        \n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "    \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.18397887323943662  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9894366197183099  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8265845070422535  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "# Validating training dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='sd',eta=0.9, regularization='none', iterations=10, C=0.01)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.8160211267605634  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.01056338028169014  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.17341549295774647  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(optimization='sd',eta=0.9, regularization='none', iterations=10, C=0.99)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Optimization Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.1505281690140845  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9242957746478874  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.7931338028169014  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(optimization='newton',eta=0.9, regularization='none', iterations=10, C=0.01)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Optimizing the classifier\n",
    "#### Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Eta Performance\n",
      "Accuracy: 0.9894366197183099 Eta: 0.97\n",
      "Accuracy: 0.988556338028169 Eta: 0.65\n",
      "Accuracy: 0.9859154929577465 Eta: 0.77\n",
      "Accuracy: 0.8829225352112676 Eta: 0.45\n",
      "Accuracy: 0.8371478873239436 Eta: 0.49\n",
      "\n",
      "Top C Performance\n",
      "Accuracy: 0.9894366197183099 C: 0.005\n",
      "Accuracy: 0.9876760563380281 C: 0.009\n",
      "Accuracy: 0.9577464788732394 C: 0.017\n",
      "Accuracy: 0.8265845070422535 C: 0.005\n",
      "Accuracy: 0.8257042253521126 C: 0.009\n",
      "\n",
      "Top Regularization Performance\n",
      "Accuracy: 0.9894366197183099 Regularization: L1_L2\n",
      "Accuracy: 0.8265845070422535 Regularization: L1_L2\n",
      "Accuracy: 0.18397887323943662 Regularization: L1_L2\n",
      "\n",
      "Optimized Config - Eta: 0.97 C: 0.005 Regularization: L1_L2\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from collections import OrderedDict\n",
    "#find best eta\n",
    "performance=dict()\n",
    "for i in range(1,99,4):\n",
    "    lr = LogisticRegression(optimization=\"sd\",eta=i/100, regularization='none', iterations=50, C=.01)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/100\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print('Top Eta Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Eta:', top_performance[key])\n",
    "    \n",
    "best_eta = top_performance[list(top_performance.keys())[0]]\n",
    "#find best C\n",
    "performance=dict()\n",
    "for i in range(1,99,4):\n",
    "    lr = LogisticRegression(optimization=\"sd\",eta=best_eta, regularization='none', iterations=50, C=i/100)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/1000\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top C Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'C:', top_performance[key])\n",
    "    \n",
    "best_c = top_performance[list(top_performance.keys())[0]]\n",
    "#find best regularization term\n",
    "reg_terms = ['none','L1','L2','L1_L2']\n",
    "performance=dict()\n",
    "for reg in reg_terms:\n",
    "    lr = LogisticRegression(optimization=\"sd\",eta=best_eta, regularization=reg, iterations=50, C=best_c)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top Regularization Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Regularization:', top_performance[key])\n",
    "    \n",
    "best_reg = top_performance[list(top_performance.keys())[0]]\n",
    "print()\n",
    "print(\"Optimized Config - Eta:\",best_eta,\"C:\",best_c,\"Regularization:\",best_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.18397887323943662  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9894366197183099  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8265845070422535  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "#optimized performance\n",
    "lr = LogisticRegression(optimization=\"sd\",eta=best_eta, regularization=best_reg, iterations=50, C=best_c)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,j],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Eta Performance\n",
      "Accuracy: 0.9894366197183099 Eta: 0.97\n",
      "Accuracy: 0.983274647887324 Eta: 0.33\n",
      "Accuracy: 0.9797535211267606 Eta: 0.17\n",
      "Accuracy: 0.9762323943661971 Eta: 0.61\n",
      "Accuracy: 0.9683098591549296 Eta: 0.25\n",
      "\n",
      "Top C Performance\n",
      "Accuracy: 0.9894366197183099 C: 0.097\n",
      "Accuracy: 0.8265845070422535 C: 0.057\n",
      "Accuracy: 0.8257042253521126 C: 0.025\n",
      "Accuracy: 0.8160211267605634 C: 0.097\n",
      "Accuracy: 0.7913732394366197 C: 0.005\n",
      "\n",
      "Top Regularization Performance\n",
      "Accuracy: 0.9894366197183099 Regularization: none\n",
      "Accuracy: 0.988556338028169 Regularization: L1_L2\n",
      "Accuracy: 0.8265845070422535 Regularization: L1_L2\n",
      "Accuracy: 0.8177816901408451 Regularization: L1\n",
      "Accuracy: 0.7200704225352113 Regularization: L1\n",
      "\n",
      "Optimized Config - Eta: 0.97 C: 0.097 Regularization: none\n"
     ]
    }
   ],
   "source": [
    "#find best eta\n",
    "performance=dict()\n",
    "for i in range(1,99,4):\n",
    "    lr = LogisticRegression(optimization=\"sgd\",eta=i/100, regularization='none', iterations=50, C=.01)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/100\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print('Top Eta Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Eta:', top_performance[key])\n",
    "    \n",
    "best_eta = top_performance[list(top_performance.keys())[0]]\n",
    "#find best C\n",
    "performance=dict()\n",
    "for i in range(1,99,4):\n",
    "    lr = LogisticRegression(optimization=\"sgd\",eta=best_eta, regularization='none', iterations=50, C=i/100)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/1000\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top C Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'C:', top_performance[key])\n",
    "    \n",
    "best_c = top_performance[list(top_performance.keys())[0]]\n",
    "#find best regularization term\n",
    "reg_terms = ['none','L1','L2','L1_L2']\n",
    "performance=dict()\n",
    "for reg in reg_terms:\n",
    "    lr = LogisticRegression(optimization=\"sgd\",eta=best_eta, regularization=reg, iterations=50, C=best_c)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top Regularization Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Regularization:', top_performance[key])\n",
    "    \n",
    "best_reg = top_performance[list(top_performance.keys())[0]]\n",
    "print()\n",
    "print(\"Optimized Config - Eta:\",best_eta,\"C:\",best_c,\"Regularization:\",best_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.8133802816901409  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9894366197183099  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8265845070422535  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "#optimized performance\n",
    "lr = LogisticRegression(optimization=\"sgd\",eta=best_eta, regularization=best_reg, iterations=50, C=best_c)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,j],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Eta Performance\n",
      "Accuracy: 0.9551056338028169 Eta: 0.01\n",
      "Accuracy: 0.9242957746478874 Eta: 0.99\n",
      "Accuracy: 0.8107394366197183 Eta: 0.01\n",
      "Accuracy: 0.7931338028169014 Eta: 0.99\n",
      "Accuracy: 0.16549295774647887 Eta: 0.01\n",
      "\n",
      "Top C Performance\n",
      "Accuracy: 0.9551056338028169 C: 0.5\n",
      "Accuracy: 0.954225352112676 C: 0.99\n",
      "Accuracy: 0.9524647887323944 C: 0.75\n",
      "Accuracy: 0.9242957746478874 C: 0.99\n",
      "Accuracy: 0.8107394366197183 C: 0.5\n",
      "\n",
      "Top Regularization Performance\n",
      "Accuracy: 0.9551056338028169 Regularization: L1_L2\n",
      "Accuracy: 0.8107394366197183 Regularization: L1_L2\n",
      "Accuracy: 0.16549295774647887 Regularization: L1_L2\n",
      "\n",
      "Optimized Config - Eta: 0.01 C: 0.5 Regularization: L1_L2\n"
     ]
    }
   ],
   "source": [
    "#values were tested at a more sparse range to make up for extended runtime with newton's method\n",
    "#find best eta\n",
    "performance=dict()\n",
    "#eta .01\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=.01, regularization='none', iterations=50, C=.01)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .01\n",
    "#eta .25\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=.25, regularization='none', iterations=50, C=.01)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .25\n",
    "#eta .5  \n",
    "lr = LogisticRegression(optimization=\"newton\",eta=.5, regularization='none', iterations=50, C=.01)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .5\n",
    "#eta .75\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=.75, regularization='none', iterations=50, C=.01)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .75\n",
    "#eta .99\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=.99, regularization='none', iterations=50, C=.01)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .99\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print('Top Eta Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Eta:', top_performance[key])\n",
    "    \n",
    "best_eta = top_performance[list(top_performance.keys())[0]]\n",
    "#find best C\n",
    "#c .01\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization='none', iterations=50, C=.01)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .01\n",
    "#c .25\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization='none', iterations=50, C=.25)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .25\n",
    "#c .5  \n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization='none', iterations=50, C=.5)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .5\n",
    "#c .75\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization='none', iterations=50, C=.75)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .75\n",
    "#c .99\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization='none', iterations=50, C=.99)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .99\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top C Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'C:', top_performance[key])\n",
    "    \n",
    "best_c = top_performance[list(top_performance.keys())[0]]\n",
    "#find best regularization term\n",
    "reg_terms = ['none','L1','L2','L1_L2']\n",
    "performance=dict()\n",
    "for reg in reg_terms:\n",
    "    lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization=reg, iterations=50, C=best_c)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top Regularization Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Regularization:', top_performance[key])\n",
    "    \n",
    "best_reg = top_performance[list(top_performance.keys())[0]]\n",
    "print()\n",
    "print(\"Optimized Config - Eta:\",best_eta,\"C:\",best_c,\"Regularization:\",best_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.16549295774647887  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9551056338028169  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8107394366197183  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "#optimized performance\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization=best_reg, iterations=50, C=best_c)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,j],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finding the optimal performance for each optimization technique, we can see that stochastic gradient descent has the most reliable results with an eta value of .97, a C value of .097, and using both L1 and L2 regularizations. Our approach to selecting these parameters is justified because we investigated each combination of the parameters. We were able to find reliable eta and C values by testing a range of values between .01 and .99. While we couldn't test a larger range of values due to runtime constraints, the utilized range provided a reasonable variety of tests. After finding these values, each regularization term tested to find which worked best.\n",
    "\n",
    "Our approach to finding parameters did not lead to data snooping because we used eta and C values after iterating through a range of values to find the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Scikit-learn Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.7570422535211268  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9894366197183099  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8265845070422535  -  Diabetes\n",
      "Average runtime: 0.0023299853006998696s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#optimized performance for stochastic gradient descent\n",
    "lr = LogisticRegression(optimization=\"sgd\",eta=.97, regularization='L1_L2', iterations=50, C=.097)\n",
    "total_time=0\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    start_time = time.time()\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    total_time += (time.time() - start_time)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,j],yhat), \" - \", col)\n",
    "    \n",
    "print(\"Average runtime: {}s\".format(float(total_time)/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.8204225352112676  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9894366197183099  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8265845070422535  -  Diabetes\n",
      "Average runtime: 0.01701521873474121s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "total_time=0\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train[:,j])\n",
    "    clf.predict(X_test)\n",
    "    clf.predict_proba(X_test)\n",
    "    total_time += (time.time() - start_time)\n",
    "    print(\"Accuracy of Testing Dataset: \", clf.score(X_test,y_test[:,j]), ' - ', col)\n",
    "\n",
    "print(\"Average runtime: {}s\".format(float(total_time)/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to our \"best\" logistic regression optimization procedure: stochastic gradient descent, the procedure used in scikit-learn has a better classification performance. Our classifier using stochastic gradient descent was able to outperform scikit-learn in regards to training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit Learn has a comparatively superior implementation for logistic regression for multiple reasons. One of the main reasons is that it is written in C++, as opposed to Python, which is essentially a wrapper for C in most instances. Moreover, since Scikit is an external libray, the user, for the most part, need not worry about the various packages and dependencies necessary to effectively implement the programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exceptional Work\n",
    "###### Option 1: Optimizing logistic regression by implementing MSE and using Newton's method to update the values of  \"w\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# inherit from base class\n",
    "class BinaryLogisticRegression_MSE(BinaryLogisticRegressionBase):\n",
    "    # private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # creating ability to choose optimization technique\n",
    "    def _get_gradient(self,X,y):\n",
    "        \n",
    "        if self.opt == 'sd': \n",
    "            gradient = self.steepest_descent\n",
    "        elif self.opt == 'sgd': \n",
    "            gradient = self.stochastic_gradient_descent\n",
    "        elif self.opt == 'newton': \n",
    "            gradient = self.newton\n",
    "        else:\n",
    "            print('No optimization chosen')\n",
    "        return gradient(X,y)\n",
    "    \n",
    "    def steepest_descent(self,X,y):\n",
    "    \n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "    \n",
    "    def stochastic_gradient_descent(self,X,y):\n",
    "        \n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "    \n",
    "    def newton(self,X,y):\n",
    "        \n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "        \n",
    "        mse = np.zeros((len(y,)))\n",
    "        for i in range(len(y)):\n",
    "            mse[i] = brier_score_loss(y, g)\n",
    "        \n",
    "        gradient = (np.sum(X * mse[:,np.newaxis], axis=0)) / len(mse)\n",
    "\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += self.C * self._get_reg_gradient()\n",
    "        return pinv(hessian) @ gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # regularization methods\n",
    "    def _get_reg_gradient(self):\n",
    "        \n",
    "        if self.reg == 'none':\n",
    "            return self.w_[1:]\n",
    "        elif self.reg == 'L1':\n",
    "            return np.sign(self.w_[1:])\n",
    "        elif self.reg == 'L2':\n",
    "            return -2 * self.w_[1:]\n",
    "        elif self.reg == 'L1_L2':\n",
    "            return -2 * self.w_[1:] + np.sign(self.w_[1:])    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        for _ in range(int(self.iters)):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression_MSE:\n",
    "    \n",
    "    def __init__(self, eta, iterations, optimization, regularization, C=0):\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.C = C\n",
    "        self.encodings = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        self.classifiers_ = []\n",
    "        for i in range(len(y[0,:])):\n",
    "            blr = BinaryLogisticRegression_MSE(self.eta, self.iters, self.opt, self.reg, self.C)\n",
    "            blr.fit(X,y[:,i])\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # saving weights\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        \n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "    \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.1505281690140845  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9242957746478874  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.7931338028169014  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "# Validating training dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='newton',eta=0.9, regularization='none', iterations=10, C=0)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.8160211267605634  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.01056338028169014  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.17341549295774647  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "# Validating training dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression_MSE(optimization='newton',eta=0.9, regularization='none', iterations=10, C=0)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we attempt to implement mean square error (MSE) to optimize logistic regression. We use scikit's brier score function so that we can calculate a mean squared difference between the predicted probability and the actual outcome. It is clear that there is a noticeable drop in the accuracy of 2 of the 3 target features, therefore indicating that MSE is not better than binary cross entropy when optimizing a logistic regression operator. This is likely due to the fact that the MSE loss function is not convex. As a result, you are more likely to get stuck at a local minimum instead of reaching the global minimum, which drastically reduces the accuracy of the results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
