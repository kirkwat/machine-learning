{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Three: Extending Logistic Regression\n",
    "\n",
    "Team: Miro Ronac, Kirk Watson, Brandon Vincitore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation and Overview\n",
    "\n",
    "### 1.1 Task and Use-case\n",
    "\n",
    "This data can be useful in identifying prediabetes or diabetes in patients and assisting doctors with making accurate observations from a variety of health indicators.\n",
    "\n",
    "Every year, the CDC collects data from a health-related telephone survey called the Behavioral Risk Factor Surveillance System. The data gathered from these surveys include information on “health-related risk behaviors, chronic health conditions, and use of preventive services.” This dataset focuses on responses from 2015 and diabetes, a “prevalent chronic disease in the United States.”\n",
    "\n",
    "Ultimately, the ability to identify a patient with prediabetes or diabetes with increased efficiency and accuracy is the intention of analyzing this dataset. With this capability, a diabetes diagnosis can be reached at a faster rate compared to when a human makes the diagnosis. Diabetes is extremely common in the US as about 1 in 10 Americans have diabetes, and [about 1 in 5 people with diabetes don’t know they have it](https://www.cdc.gov/diabetes/library/spotlights/diabetes-facts-stats.html#:~:text=37.3%20million%20Americans%E2%80%94about%201,t%20know%20they%20have%20it.). In addition, 1 in 3 Americans have prediabetes, and [more than 8 in 10 adults with prediabetes don’t know they have it](https://www.cdc.gov/diabetes/library/spotlights/diabetes-facts-stats.html#:~:text=37.3%20million%20Americans%E2%80%94about%201,t%20know%20they%20have%20it.). Using this classifier, patients that might be at risk of diabetes can be brought to a doctor’s attention at a higher rate allowing for earlier medical care and attention.\n",
    "\n",
    "Dataset Source: https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset\n",
    "\n",
    "### 1.2 Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 253680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
       "0           0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
       "1           0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
       "2           0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
       "3           0.0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
       "4           0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
       "\n",
       "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
       "0                   0.0           0.0     0.0  ...            1.0   \n",
       "1                   0.0           1.0     0.0  ...            0.0   \n",
       "2                   0.0           0.0     1.0  ...            1.0   \n",
       "3                   0.0           1.0     1.0  ...            1.0   \n",
       "4                   0.0           1.0     1.0  ...            1.0   \n",
       "\n",
       "   NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  \\\n",
       "0          0.0      5.0      18.0      15.0       1.0  0.0   9.0        4.0   \n",
       "1          1.0      3.0       0.0       0.0       0.0  0.0   7.0        6.0   \n",
       "2          1.0      5.0      30.0      30.0       1.0  0.0   9.0        4.0   \n",
       "3          0.0      2.0       0.0       0.0       0.0  0.0  11.0        3.0   \n",
       "4          0.0      2.0       3.0       0.0       0.0  0.0  11.0        5.0   \n",
       "\n",
       "   Income  \n",
       "0     3.0  \n",
       "1     1.0  \n",
       "2     8.0  \n",
       "3     6.0  \n",
       "4     4.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('diabetes_012_health_indicators_BRFSS2015.csv')\n",
    "print('Size of dataset:', df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 5680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NoDiabetes</th>\n",
       "      <th>PreDiabetes</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NoDiabetes  PreDiabetes  Diabetes  HighBP  HighChol  CholCheck  BMI  \\\n",
       "0           1            0         0       1         1          1   40   \n",
       "1           1            0         0       0         0          0   25   \n",
       "2           1            0         0       1         1          1   28   \n",
       "3           1            0         0       1         0          1   27   \n",
       "4           1            0         0       1         1          1   24   \n",
       "\n",
       "   Smoker  Stroke  HeartDiseaseorAttack  ...  AnyHealthcare  NoDocbcCost  \\\n",
       "0       1       0                     0  ...              1            0   \n",
       "1       1       0                     0  ...              0            1   \n",
       "2       0       0                     0  ...              1            1   \n",
       "3       0       0                     0  ...              1            0   \n",
       "4       0       0                     0  ...              1            0   \n",
       "\n",
       "   GenHlth  MentHlth  PhysHlth  DiffWalk  Sex  Age  Education  Income  \n",
       "0        5        18        15         1    0    9          4       3  \n",
       "1        3         0         0         0    0    7          6       1  \n",
       "2        5        30        30         1    0    9          4       8  \n",
       "3        2         0         0         0    0   11          3       6  \n",
       "4        2         3         0         0    0   11          5       4  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating target features by modifying first column in original dataframe such that we have 3 features consisting of binary values for\n",
    "# no diabetes, prediabetes, and diabetes where 0 is False and 1 is True\n",
    "target_array = np.zeros((len(df),4))\n",
    "for i in range(len(df)):\n",
    "    target_array[i,0] = df['Diabetes_012'].values[i]\n",
    "for i in range(len(target_array)):\n",
    "    # no diabetes\n",
    "    if target_array[i,0] == 0:\n",
    "        target_array[i,1] = 1\n",
    "    # prediabetes\n",
    "    if target_array[i,0] == 1:\n",
    "        target_array[i,2] = 1\n",
    "    # diabetes\n",
    "    if target_array[i,0] == 2:\n",
    "        target_array[i,3] = 1\n",
    "\n",
    "# Adding new target columns to original dataframe\n",
    "target_columns = ['NoDiabetes', 'PreDiabetes', 'Diabetes']\n",
    "for i in range(target_array.shape[1]-1):\n",
    "    df.insert(i, target_columns[i], target_array[:,1:][:,i], True)\n",
    "df_target = df.drop('Diabetes_012', axis=1)\n",
    "\n",
    "columns = list(df_target.columns)\n",
    "for col in columns:\n",
    "    df_target[col] = df_target[col].astype(int)\n",
    "    \n",
    "df_target.drop(df_target.tail(248000).index,inplace = True)\n",
    "print('Size of dataset:', df_target.shape[0])\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataset Training and Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['NoDiabetes', 'PreDiabetes', 'Diabetes']\n",
    "for col in targets:\n",
    "    columns.remove(col)\n",
    "    \n",
    "# Splitting dataset\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "train, test = tts(df_target, test_size=.20, random_state=42, shuffle=True)\n",
    "\n",
    "X_test  = test[columns].to_numpy()\n",
    "X_train = train[columns].to_numpy()\n",
    "y_test = test[targets].to_numpy()\n",
    "y_train = train[targets].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a large dataset of over 250,000 features, an 80/20 split is sufficient. Our classifier has plenty of data to use for training and also has plenty of data to use for training. We could comformtably move our split to 75/25 or 70/30 if we desired more opportunities to test our classifier. With such a large dataset, we could also divide the dataset with an additional validation set. With a validation set, we can use this set for more frequent model evaulations and save the testing dataset for a final unbiased evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling\n",
    "### Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta=0.01, iterations=50, optimization='sd', regularization='none', C=0):\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) # return the actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    # private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # creating ability to choose optimization technique\n",
    "    def _get_gradient(self,X,y):\n",
    "        \n",
    "        gradient = None\n",
    "        if self.opt == 'sd': \n",
    "            gradient = self.steepest_descent\n",
    "        elif self.opt == 'sgd': \n",
    "            gradient = self.stochastic_gradient_descent\n",
    "        elif self.opt == 'newton': \n",
    "            gradient = self.newton\n",
    "        else:\n",
    "            print('No optimization chosen')\n",
    "        return gradient(X,y)\n",
    "    \n",
    "    def steepest_descent(self,X,y):\n",
    "    \n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "    \n",
    "    def stochastic_gradient_descent(self,X,y):\n",
    "        \n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "    \n",
    "    def newton(self,X,y):\n",
    "        \n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return pinv(hessian) @ gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # regularization methods\n",
    "    def _get_reg_gradient(self):\n",
    "        \n",
    "        if self.reg == 'none':\n",
    "            return self.w_[1:]\n",
    "        elif self.reg == 'L1':\n",
    "            return np.sign(self.w_[1:])\n",
    "        elif self.reg == 'L2':\n",
    "            return -2 * self.w_[1:]\n",
    "        elif self.reg == 'L1_L2':\n",
    "            return -2 * self.w_[1:] + np.sign(self.w_[1:])    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        for _ in range(int(self.iters)):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, eta, iterations, optimization, regularization, C=0):\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.C = C\n",
    "        self.encodings = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        self.classifiers_ = []\n",
    "        for i in range(len(y[0,:])):\n",
    "            blr = BinaryLogisticRegression(self.eta, self.iters, self.opt, self.reg, self.C)\n",
    "            blr.fit(X,y[:,i])\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # saving weights\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        \n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "    \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.18397887323943662  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9894366197183099  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8265845070422535  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "# Validating training dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='sd',eta=0.9, regularization='none', iterations=10, C=0.01)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.8160211267605634  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.01056338028169014  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.17341549295774647  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='sd',eta=0.9, regularization='none', iterations=10, C=0.99)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Optimization Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.7517605633802817  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.891725352112676  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8265845070422535  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(optimization='sgd',eta=0.9, regularization='none', iterations=10, C=0.01)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Optimizing the classifier\n",
    "#### Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Eta Performance\n",
      "Accuracy: 0.9894366197183099 Eta: 0.24\n",
      "Accuracy: 0.9427816901408451 Eta: 0.06\n",
      "Accuracy: 0.9251760563380281 Eta: 0.08\n",
      "Accuracy: 0.9110915492957746 Eta: 0.07\n",
      "Accuracy: 0.8433098591549296 Eta: 0.1\n",
      "\n",
      "Top C Performance\n",
      "Accuracy: 0.9894366197183099 C: 0.098\n",
      "Accuracy: 0.988556338028169 C: 0.001\n",
      "Accuracy: 0.9876760563380281 C: 0.018\n",
      "Accuracy: 0.9867957746478874 C: 0.034\n",
      "Accuracy: 0.983274647887324 C: 0.0\n",
      "\n",
      "Top Regularization Performance\n",
      "Accuracy: 0.9894366197183099 Regularization: L1_L2\n",
      "Accuracy: 0.8265845070422535 Regularization: L1_L2\n",
      "Accuracy: 0.18397887323943662 Regularization: L1_L2\n",
      "\n",
      "Optimized Config - Eta: 0.24 C: 0.098 Regularization: L1_L2\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from collections import OrderedDict\n",
    "#find best eta\n",
    "performance=dict()\n",
    "for i in range(25):\n",
    "    lr = LogisticRegression(optimization=\"sd\",eta=i/100, regularization='none', iterations=50, C=.01)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/100\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print('Top Eta Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Eta:', top_performance[key])\n",
    "    \n",
    "best_eta = top_performance[list(top_performance.keys())[0]]\n",
    "#find best C\n",
    "performance=dict()\n",
    "for i in range(100):\n",
    "    lr = LogisticRegression(optimization=\"sd\",eta=best_eta, regularization='none', iterations=50, C=i/1000)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/1000\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top C Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'C:', top_performance[key])\n",
    "    \n",
    "best_c = top_performance[list(top_performance.keys())[0]]\n",
    "#find best regularization term\n",
    "reg_terms = ['none','L1','L2','L1_L2']\n",
    "performance=dict()\n",
    "for reg in reg_terms:\n",
    "    lr = LogisticRegression(optimization=\"sd\",eta=best_eta, regularization=reg, iterations=50, C=best_c)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top Regularization Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Regularization:', top_performance[key])\n",
    "    \n",
    "best_reg = top_performance[list(top_performance.keys())[0]]\n",
    "print()\n",
    "print(\"Optimized Config - Eta:\",best_eta,\"C:\",best_c,\"Regularization:\",best_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.18397887323943662  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9894366197183099  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8265845070422535  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "#optimized performance\n",
    "lr = LogisticRegression(optimization=\"sd\",eta=best_eta, regularization=best_reg, iterations=50, C=best_c)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,j],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Eta Performance\n",
      "Accuracy: 0.9894366197183099 Eta: 0.21\n",
      "Accuracy: 0.988556338028169 Eta: 0.1\n",
      "Accuracy: 0.9753521126760564 Eta: 0.11\n",
      "Accuracy: 0.9612676056338029 Eta: 0.04\n",
      "Accuracy: 0.954225352112676 Eta: 0.02\n",
      "\n",
      "Top C Performance\n",
      "Accuracy: 0.9894366197183099 C: 0.099\n",
      "Accuracy: 0.988556338028169 C: 0.095\n",
      "Accuracy: 0.9876760563380281 C: 0.042\n",
      "Accuracy: 0.983274647887324 C: 0.02\n",
      "Accuracy: 0.9806338028169014 C: 0.037\n",
      "\n",
      "Top Regularization Performance\n",
      "Accuracy: 0.9894366197183099 Regularization: L2\n",
      "Accuracy: 0.9850352112676056 Regularization: none\n",
      "Accuracy: 0.8767605633802817 Regularization: L1_L2\n",
      "Accuracy: 0.8274647887323944 Regularization: L1\n",
      "Accuracy: 0.8265845070422535 Regularization: L1_L2\n",
      "\n",
      "Optimized Config - Eta: 0.21 C: 0.099 Regularization: L2\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from collections import OrderedDict\n",
    "#find best eta\n",
    "performance=dict()\n",
    "for i in range(25):\n",
    "    lr = LogisticRegression(optimization=\"sgd\",eta=i/100, regularization='none', iterations=50, C=.01)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/100\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print('Top Eta Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Eta:', top_performance[key])\n",
    "    \n",
    "best_eta = top_performance[list(top_performance.keys())[0]]\n",
    "#find best C\n",
    "performance=dict()\n",
    "for i in range(100):\n",
    "    lr = LogisticRegression(optimization=\"sgd\",eta=best_eta, regularization='none', iterations=50, C=i/1000)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/1000\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top C Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'C:', top_performance[key])\n",
    "    \n",
    "best_c = top_performance[list(top_performance.keys())[0]]\n",
    "#find best regularization term\n",
    "reg_terms = ['none','L1','L2','L1_L2']\n",
    "performance=dict()\n",
    "for reg in reg_terms:\n",
    "    lr = LogisticRegression(optimization=\"sgd\",eta=best_eta, regularization=reg, iterations=50, C=best_c)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top Regularization Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Regularization:', top_performance[key])\n",
    "    \n",
    "best_reg = top_performance[list(top_performance.keys())[0]]\n",
    "print()\n",
    "print(\"Optimized Config - Eta:\",best_eta,\"C:\",best_c,\"Regularization:\",best_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized performance\n",
    "lr = LogisticRegression(optimization=\"sgd\",eta=best_eta, regularization=best_reg, iterations=50, C=best_c)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,j],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import OrderedDict\n",
    "#find best eta\n",
    "performance=dict()\n",
    "for i in range(25):\n",
    "    lr = LogisticRegression(optimization=\"newton\",eta=i/100, regularization='none', iterations=50, C=.01)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/100\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print('Top Eta Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Eta:', top_performance[key])\n",
    "    \n",
    "best_eta = top_performance[list(top_performance.keys())[0]]\n",
    "#find best C\n",
    "performance=dict()\n",
    "for i in range(100):\n",
    "    lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization='none', iterations=50, C=i/1000)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/1000\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top C Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'C:', top_performance[key])\n",
    "    \n",
    "best_c = top_performance[list(top_performance.keys())[0]]\n",
    "#find best regularization term\n",
    "reg_terms = ['none','L1','L2','L1_L2']\n",
    "performance=dict()\n",
    "for reg in reg_terms:\n",
    "    lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization=reg, iterations=50, C=best_c)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top Regularization Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Regularization:', top_performance[key])\n",
    "    \n",
    "best_reg = top_performance[list(top_performance.keys())[0]]\n",
    "print()\n",
    "print(\"Optimized Config - Eta:\",best_eta,\"C:\",best_c,\"Regularization:\",best_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#optimized performance\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization=best_reg, iterations=50, C=best_c)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,j],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skikit-learn Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exceptional Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
