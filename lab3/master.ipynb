{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Three: Extending Logistic Regression\n",
    "\n",
    "Team: Miro Ronac, Kirk Watson, Brandon Vincitore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation and Overview\n",
    "\n",
    "### 1.1 Task and Use-case\n",
    "\n",
    "This data can be useful in identifying prediabetes or diabetes in patients and assisting doctors with making accurate observations from a variety of health indicators.\n",
    "\n",
    "Every year, the CDC collects data from a health-related telephone survey called the Behavioral Risk Factor Surveillance System. The data gathered from these surveys include information on “health-related risk behaviors, chronic health conditions, and use of preventive services.” This dataset focuses on responses from 2015 and diabetes, a “prevalent chronic disease in the United States.”\n",
    "\n",
    "Ultimately, the ability to identify a patient with prediabetes or diabetes with increased efficiency and accuracy is the intention of analyzing this dataset. With this capability, a diabetes diagnosis can be reached at a faster rate compared to when a human makes the diagnosis. Diabetes is extremely common in the US as about 1 in 10 Americans have diabetes, and [about 1 in 5 people with diabetes don’t know they have it](https://www.cdc.gov/diabetes/library/spotlights/diabetes-facts-stats.html#:~:text=37.3%20million%20Americans%E2%80%94about%201,t%20know%20they%20have%20it.). In addition, 1 in 3 Americans have prediabetes, and [more than 8 in 10 adults with prediabetes don’t know they have it](https://www.cdc.gov/diabetes/library/spotlights/diabetes-facts-stats.html#:~:text=37.3%20million%20Americans%E2%80%94about%201,t%20know%20they%20have%20it.). Using this classifier, patients that might be at risk of diabetes can be brought to a doctor’s attention at a higher rate allowing for earlier medical care and attention.\n",
    "\n",
    "Dataset Source: https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset\n",
    "\n",
    "### 1.2 Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 253680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Diabetes_012</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
       "0           0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
       "1           0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
       "2           0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
       "3           0.0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
       "4           0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
       "\n",
       "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
       "0                   0.0           0.0     0.0  ...            1.0   \n",
       "1                   0.0           1.0     0.0  ...            0.0   \n",
       "2                   0.0           0.0     1.0  ...            1.0   \n",
       "3                   0.0           1.0     1.0  ...            1.0   \n",
       "4                   0.0           1.0     1.0  ...            1.0   \n",
       "\n",
       "   NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  \\\n",
       "0          0.0      5.0      18.0      15.0       1.0  0.0   9.0        4.0   \n",
       "1          1.0      3.0       0.0       0.0       0.0  0.0   7.0        6.0   \n",
       "2          1.0      5.0      30.0      30.0       1.0  0.0   9.0        4.0   \n",
       "3          0.0      2.0       0.0       0.0       0.0  0.0  11.0        3.0   \n",
       "4          0.0      2.0       3.0       0.0       0.0  0.0  11.0        5.0   \n",
       "\n",
       "   Income  \n",
       "0     3.0  \n",
       "1     1.0  \n",
       "2     8.0  \n",
       "3     6.0  \n",
       "4     4.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('diabetes_012_health_indicators_BRFSS2015.csv')\n",
    "print('Size of dataset:', df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset: 5680\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NoDiabetes</th>\n",
       "      <th>PreDiabetes</th>\n",
       "      <th>Diabetes</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>...</th>\n",
       "      <th>AnyHealthcare</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   NoDiabetes  PreDiabetes  Diabetes  HighBP  HighChol  CholCheck  BMI  \\\n",
       "0           1            0         0       1         1          1   40   \n",
       "1           1            0         0       0         0          0   25   \n",
       "2           1            0         0       1         1          1   28   \n",
       "3           1            0         0       1         0          1   27   \n",
       "4           1            0         0       1         1          1   24   \n",
       "\n",
       "   Smoker  Stroke  HeartDiseaseorAttack  ...  AnyHealthcare  NoDocbcCost  \\\n",
       "0       1       0                     0  ...              1            0   \n",
       "1       1       0                     0  ...              0            1   \n",
       "2       0       0                     0  ...              1            1   \n",
       "3       0       0                     0  ...              1            0   \n",
       "4       0       0                     0  ...              1            0   \n",
       "\n",
       "   GenHlth  MentHlth  PhysHlth  DiffWalk  Sex  Age  Education  Income  \n",
       "0        5        18        15         1    0    9          4       3  \n",
       "1        3         0         0         0    0    7          6       1  \n",
       "2        5        30        30         1    0    9          4       8  \n",
       "3        2         0         0         0    0   11          3       6  \n",
       "4        2         3         0         0    0   11          5       4  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating target features by modifying first column in original dataframe such that we have 3 features consisting of binary values for\n",
    "# no diabetes, prediabetes, and diabetes where 0 is False and 1 is True\n",
    "target_array = np.zeros((len(df),4))\n",
    "for i in range(len(df)):\n",
    "    target_array[i,0] = df['Diabetes_012'].values[i]\n",
    "for i in range(len(target_array)):\n",
    "    # no diabetes\n",
    "    if target_array[i,0] == 0:\n",
    "        target_array[i,1] = 1\n",
    "    # prediabetes\n",
    "    if target_array[i,0] == 1:\n",
    "        target_array[i,2] = 1\n",
    "    # diabetes\n",
    "    if target_array[i,0] == 2:\n",
    "        target_array[i,3] = 1\n",
    "\n",
    "# Adding new target columns to original dataframe\n",
    "target_columns = ['NoDiabetes', 'PreDiabetes', 'Diabetes']\n",
    "for i in range(target_array.shape[1]-1):\n",
    "    df.insert(i, target_columns[i], target_array[:,1:][:,i], True)\n",
    "df_target = df.drop('Diabetes_012', axis=1)\n",
    "\n",
    "columns = list(df_target.columns)\n",
    "for col in columns:\n",
    "    df_target[col] = df_target[col].astype(int)\n",
    "    \n",
    "df_target.drop(df_target.tail(248000).index,inplace = True)\n",
    "print('Size of dataset:', df_target.shape[0])\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dataset Training and Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['NoDiabetes', 'PreDiabetes', 'Diabetes']\n",
    "for col in targets:\n",
    "    columns.remove(col)\n",
    "    \n",
    "# Splitting dataset\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "train, test = tts(df_target, test_size=.20, random_state=42, shuffle=True)\n",
    "\n",
    "X_test  = test[columns].to_numpy()\n",
    "X_train = train[columns].to_numpy()\n",
    "y_test = test[targets].to_numpy()\n",
    "y_train = train[targets].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a large dataset of over 250,000 instances, an 80/20 split is sufficient. Our classifier has plenty of data to use for training and also has plenty of data to use for training. We could comformtably move our split to 75/25 or 70/30 if we desired more opportunities to test our classifier. With such a large dataset, we could also divide the dataset with an additional validation set. With a validation set, we can use this set for more frequent model evaulations and save the testing dataset for a final unbiased evaluation. \n",
    "\n",
    "UPDATE: We reduced the size of the dataset to allow for reasonable runtime. With 5680 instances, the dataset is large enough to the above to still be valid. However, we do not have as much freedom to possible create a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling\n",
    "### Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta=0.01, iterations=50, optimization='sd', regularization='none', C=0):\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self, X, add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) # return the actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    # private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # creating ability to choose optimization technique\n",
    "    def _get_gradient(self,X,y):\n",
    "        \n",
    "        gradient = None\n",
    "        if self.opt == 'sd': \n",
    "            gradient = self.steepest_descent\n",
    "        elif self.opt == 'sgd': \n",
    "            gradient = self.stochastic_gradient_descent\n",
    "        elif self.opt == 'newton': \n",
    "            gradient = self.newton\n",
    "        else:\n",
    "            print('No optimization chosen')\n",
    "        return gradient(X,y)\n",
    "    \n",
    "    def steepest_descent(self,X,y):\n",
    "    \n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "    \n",
    "    def stochastic_gradient_descent(self,X,y):\n",
    "        \n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "    \n",
    "    def newton(self,X,y):\n",
    "        \n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        \n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        \n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += self.C * self._get_reg_gradient()\n",
    "        return pinv(hessian) @ gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # regularization methods\n",
    "    def _get_reg_gradient(self):\n",
    "        \n",
    "        if self.reg == 'none':\n",
    "            return self.w_[1:]\n",
    "        elif self.reg == 'L1':\n",
    "            return np.sign(self.w_[1:])\n",
    "        elif self.reg == 'L2':\n",
    "            return -2 * self.w_[1:]\n",
    "        elif self.reg == 'L1_L2':\n",
    "            return -2 * self.w_[1:] + np.sign(self.w_[1:])    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        for _ in range(int(self.iters)):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, eta, iterations, optimization, regularization, C=0):\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.C = C\n",
    "        self.encodings = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        self.classifiers_ = []\n",
    "        for i in range(len(y[0,:])):\n",
    "            blr = BinaryLogisticRegression(self.eta, self.iters, self.opt, self.reg, self.C)\n",
    "            blr.fit(X,y[:,i])\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # saving weights\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        \n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "    \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.18397887323943662  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9894366197183099  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8265845070422535  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "# Validating training dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='sd',eta=0.9, regularization='none', iterations=10, C=0.01)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.8160211267605634  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.01056338028169014  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.17341549295774647  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='sd',eta=0.9, regularization='none', iterations=10, C=0.99)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Optimization Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.1505281690140845  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9242957746478874  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.7931338028169014  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(optimization='newton',eta=0.9, regularization='none', iterations=10, C=0.01)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Optimizing the classifier\n",
    "#### Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Eta Performance\n",
      "Accuracy: 0.9894366197183099 Eta: 0.97\n",
      "Accuracy: 0.988556338028169 Eta: 0.65\n",
      "Accuracy: 0.9859154929577465 Eta: 0.77\n",
      "Accuracy: 0.8829225352112676 Eta: 0.45\n",
      "Accuracy: 0.8371478873239436 Eta: 0.49\n",
      "\n",
      "Top C Performance\n",
      "Accuracy: 0.9894366197183099 C: 0.005\n",
      "Accuracy: 0.9876760563380281 C: 0.009\n",
      "Accuracy: 0.9577464788732394 C: 0.017\n",
      "Accuracy: 0.8265845070422535 C: 0.005\n",
      "Accuracy: 0.8257042253521126 C: 0.009\n",
      "\n",
      "Top Regularization Performance\n",
      "Accuracy: 0.9894366197183099 Regularization: L1_L2\n",
      "Accuracy: 0.8265845070422535 Regularization: L1_L2\n",
      "Accuracy: 0.18397887323943662 Regularization: L1_L2\n",
      "\n",
      "Optimized Config - Eta: 0.97 C: 0.005 Regularization: L1_L2\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from collections import OrderedDict\n",
    "#find best eta\n",
    "performance=dict()\n",
    "for i in range(1,99,4):\n",
    "    lr = LogisticRegression(optimization=\"sd\",eta=i/100, regularization='none', iterations=50, C=.01)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/100\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print('Top Eta Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Eta:', top_performance[key])\n",
    "    \n",
    "best_eta = top_performance[list(top_performance.keys())[0]]\n",
    "#find best C\n",
    "performance=dict()\n",
    "for i in range(1,99,4):\n",
    "    lr = LogisticRegression(optimization=\"sd\",eta=best_eta, regularization='none', iterations=50, C=i/100)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/1000\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top C Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'C:', top_performance[key])\n",
    "    \n",
    "best_c = top_performance[list(top_performance.keys())[0]]\n",
    "#find best regularization term\n",
    "reg_terms = ['none','L1','L2','L1_L2']\n",
    "performance=dict()\n",
    "for reg in reg_terms:\n",
    "    lr = LogisticRegression(optimization=\"sd\",eta=best_eta, regularization=reg, iterations=50, C=best_c)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top Regularization Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Regularization:', top_performance[key])\n",
    "    \n",
    "best_reg = top_performance[list(top_performance.keys())[0]]\n",
    "print()\n",
    "print(\"Optimized Config - Eta:\",best_eta,\"C:\",best_c,\"Regularization:\",best_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.18397887323943662  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9894366197183099  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8265845070422535  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "#optimized performance\n",
    "lr = LogisticRegression(optimization=\"sd\",eta=best_eta, regularization=best_reg, iterations=50, C=best_c)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,j],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Eta Performance\n",
      "Accuracy: 0.9894366197183099 Eta: 0.97\n",
      "Accuracy: 0.9876760563380281 Eta: 0.41\n",
      "Accuracy: 0.9867957746478874 Eta: 0.65\n",
      "Accuracy: 0.9815140845070423 Eta: 0.37\n",
      "Accuracy: 0.9709507042253521 Eta: 0.77\n",
      "\n",
      "Top C Performance\n",
      "Accuracy: 0.9894366197183099 C: 0.097\n",
      "Accuracy: 0.9797535211267606 C: 0.029\n",
      "Accuracy: 0.8873239436619719 C: 0.001\n",
      "Accuracy: 0.8265845070422535 C: 0.093\n",
      "Accuracy: 0.8160211267605634 C: 0.093\n",
      "\n",
      "Top Regularization Performance\n",
      "Accuracy: 0.9894366197183099 Regularization: L1_L2\n",
      "Accuracy: 0.9841549295774648 Regularization: L1\n",
      "Accuracy: 0.9612676056338029 Regularization: L2\n",
      "Accuracy: 0.8265845070422535 Regularization: L1_L2\n",
      "Accuracy: 0.823943661971831 Regularization: none\n",
      "\n",
      "Optimized Config - Eta: 0.97 C: 0.097 Regularization: L1_L2\n"
     ]
    }
   ],
   "source": [
    "#find best eta\n",
    "performance=dict()\n",
    "for i in range(1,99,4):\n",
    "    lr = LogisticRegression(optimization=\"sgd\",eta=i/100, regularization='none', iterations=50, C=.01)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/100\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print('Top Eta Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Eta:', top_performance[key])\n",
    "    \n",
    "best_eta = top_performance[list(top_performance.keys())[0]]\n",
    "#find best C\n",
    "performance=dict()\n",
    "for i in range(1,99,4):\n",
    "    lr = LogisticRegression(optimization=\"sgd\",eta=best_eta, regularization='none', iterations=50, C=i/100)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = i/1000\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top C Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'C:', top_performance[key])\n",
    "    \n",
    "best_c = top_performance[list(top_performance.keys())[0]]\n",
    "#find best regularization term\n",
    "reg_terms = ['none','L1','L2','L1_L2']\n",
    "performance=dict()\n",
    "for reg in reg_terms:\n",
    "    lr = LogisticRegression(optimization=\"sgd\",eta=best_eta, regularization=reg, iterations=50, C=best_c)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top Regularization Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Regularization:', top_performance[key])\n",
    "    \n",
    "best_reg = top_performance[list(top_performance.keys())[0]]\n",
    "print()\n",
    "print(\"Optimized Config - Eta:\",best_eta,\"C:\",best_c,\"Regularization:\",best_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.7799295774647887  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9894366197183099  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.733274647887324  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "#optimized performance\n",
    "lr = LogisticRegression(optimization=\"sgd\",eta=best_eta, regularization=best_reg, iterations=50, C=best_c)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,j],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Eta Performance\n",
      "Accuracy: 0.9551056338028169 Eta: 0.01\n",
      "Accuracy: 0.9242957746478874 Eta: 0.99\n",
      "Accuracy: 0.8107394366197183 Eta: 0.01\n",
      "Accuracy: 0.7931338028169014 Eta: 0.99\n",
      "Accuracy: 0.16549295774647887 Eta: 0.01\n",
      "\n",
      "Top C Performance\n",
      "Accuracy: 0.9551056338028169 C: 0.5\n",
      "Accuracy: 0.954225352112676 C: 0.99\n",
      "Accuracy: 0.9524647887323944 C: 0.75\n",
      "Accuracy: 0.9242957746478874 C: 0.99\n",
      "Accuracy: 0.8107394366197183 C: 0.5\n",
      "\n",
      "Top Regularization Performance\n",
      "Accuracy: 0.9551056338028169 Regularization: L1_L2\n",
      "Accuracy: 0.8107394366197183 Regularization: L1_L2\n",
      "Accuracy: 0.16549295774647887 Regularization: L1_L2\n",
      "\n",
      "Optimized Config - Eta: 0.01 C: 0.5 Regularization: L1_L2\n"
     ]
    }
   ],
   "source": [
    "#values were tested at a more sparse range to make up for extended runtime with newton's method\n",
    "#find best eta\n",
    "performance=dict()\n",
    "#eta .01\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=.01, regularization='none', iterations=50, C=.01)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .01\n",
    "#eta .25\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=.25, regularization='none', iterations=50, C=.01)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .25\n",
    "#eta .5  \n",
    "lr = LogisticRegression(optimization=\"newton\",eta=.5, regularization='none', iterations=50, C=.01)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .5\n",
    "#eta .75\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=.75, regularization='none', iterations=50, C=.01)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .75\n",
    "#eta .99\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=.99, regularization='none', iterations=50, C=.01)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .99\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print('Top Eta Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Eta:', top_performance[key])\n",
    "    \n",
    "best_eta = top_performance[list(top_performance.keys())[0]]\n",
    "#find best C\n",
    "#c .01\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization='none', iterations=50, C=.01)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .01\n",
    "#c .25\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization='none', iterations=50, C=.25)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .25\n",
    "#c .5  \n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization='none', iterations=50, C=.5)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .5\n",
    "#c .75\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization='none', iterations=50, C=.75)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .75\n",
    "#c .99\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization='none', iterations=50, C=.99)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = .99\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top C Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'C:', top_performance[key])\n",
    "    \n",
    "best_c = top_performance[list(top_performance.keys())[0]]\n",
    "#find best regularization term\n",
    "reg_terms = ['none','L1','L2','L1_L2']\n",
    "performance=dict()\n",
    "for reg in reg_terms:\n",
    "    lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization=reg, iterations=50, C=best_c)\n",
    "    for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "        lr.fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "        \n",
    "top_performance = OrderedDict(sorted(performance.items(),reverse=True)[:5])\n",
    "print()\n",
    "print('Top Regularization Performance')\n",
    "for key in top_performance:\n",
    "    print('Accuracy:',key,'Regularization:', top_performance[key])\n",
    "    \n",
    "best_reg = top_performance[list(top_performance.keys())[0]]\n",
    "print()\n",
    "print(\"Optimized Config - Eta:\",best_eta,\"C:\",best_c,\"Regularization:\",best_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.16549295774647887  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9551056338028169  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8107394366197183  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "#optimized performance\n",
    "lr = LogisticRegression(optimization=\"newton\",eta=best_eta, regularization=best_reg, iterations=50, C=best_c)\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,j],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finding the optimal performance for each optimization technique, we can see that stochastic gradient descent has the most reliable results with an eta value of .97, a C value of .097, and using both L1 and L2 regularizations. Our approach to selecting these parameters is justified because we investigated each combination of the parameters. We were able to find reliable eta and C values by testing a range of values between .01 and .99. While we couldn't test a larger range of values due to runtime constraints, the utilized range provided a reasonable variety of tests. After finding these values, each regularization term tested to find which worked best.\n",
    "\n",
    "Our approach to finding parameters did not lead to data snooping because we used eta and C values after iterating through a range of values to find the highest accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Skikit-learn Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.7702464788732394  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9797535211267606  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.8265845070422535  -  Diabetes\n",
      "Runtime: 0.002340078353881836s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#optimized performance for stochastic gradient descent\n",
    "lr = LogisticRegression(optimization=\"sgd\",eta=.97, regularization='L1_L2', iterations=50, C=.097)\n",
    "total_time=0\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    start_time = time.time()\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    total_time += (time.time() - start_time)\n",
    "    performance[accuracy_score(y_test[:,j],yhat)] = reg\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,j],yhat), \" - \", col)\n",
    "    \n",
    "print(\"Runtime: {}s\".format(float(total_time)/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (4544, 3) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j,col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_test[\u001b[38;5;241m0\u001b[39m,:])), targets):\n\u001b[0;32m      5\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      8\u001b[0m     clf\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1508\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1506\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1508\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1515\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1516\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:979\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    964\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    965\u001b[0m     X,\n\u001b[0;32m    966\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    976\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    977\u001b[0m )\n\u001b[1;32m--> 979\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric)\u001b[0m\n\u001b[0;32m    989\u001b[0m     y \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    990\u001b[0m         y, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    991\u001b[0m     )\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 993\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    994\u001b[0m     _assert_all_finite(y)\n\u001b[0;32m    995\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1038\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1030\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1031\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1035\u001b[0m         )\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[1;32m-> 1038\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1040\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (4544, 3) instead."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(solver='liblinear')\n",
    "for j,col in zip(range(len(y_test[0,:])), targets):\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    clf.predict(X_test)\n",
    "    clf.predict_proba(X_test)\n",
    "    print(clf.score(X_test,y_test[col]), ' - ', col)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exceptional Work\n",
    "###### Option 1: Optimizing logistic regression by implementing MSE and using Newton's method to update the values of  \"w\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "# inherit from base class\n",
    "class BinaryLogisticRegression_MSE(BinaryLogisticRegressionBase):\n",
    "    # private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # creating ability to choose optimization technique\n",
    "    def _get_gradient(self,X,y):\n",
    "        \n",
    "        if self.opt == 'sd': \n",
    "            gradient = self.steepest_descent\n",
    "        elif self.opt == 'sgd': \n",
    "            gradient = self.stochastic_gradient_descent\n",
    "        elif self.opt == 'newton': \n",
    "            gradient = self.newton\n",
    "        else:\n",
    "            print('No optimization chosen')\n",
    "        return gradient(X,y)\n",
    "    \n",
    "    def steepest_descent(self,X,y):\n",
    "    \n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "    \n",
    "    def stochastic_gradient_descent(self,X,y):\n",
    "        \n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        return gradient\n",
    "    \n",
    "    def newton(self,X,y):\n",
    "        \n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "        \n",
    "        mse = np.zeros((len(y,)))\n",
    "        for i in range(len(y)):\n",
    "            mse[i] = brier_score_loss(y, g)\n",
    "        \n",
    "        gradient = (np.sum(X * mse[:,np.newaxis], axis=0)) / len(mse)\n",
    "\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += self.C * self._get_reg_gradient()\n",
    "        return pinv(hessian) @ gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # regularization methods\n",
    "    def _get_reg_gradient(self):\n",
    "        \n",
    "        if self.reg == 'none':\n",
    "            return self.w_[1:]\n",
    "        elif self.reg == 'L1':\n",
    "            return np.sign(self.w_[1:])\n",
    "        elif self.reg == 'L2':\n",
    "            return -2 * self.w_[1:]\n",
    "        elif self.reg == 'L1_L2':\n",
    "            return -2 * self.w_[1:] + np.sign(self.w_[1:])    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        for _ in range(int(self.iters)):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression_MSE:\n",
    "    \n",
    "    def __init__(self, eta, iterations, optimization, regularization, C=0):\n",
    "        \n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.C = C\n",
    "        self.encodings = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'MultiClass Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained MultiClass Logistic Regression Object'\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \n",
    "        self.classifiers_ = []\n",
    "        for i in range(len(y[0,:])):\n",
    "            blr = BinaryLogisticRegression_MSE(self.eta, self.iters, self.opt, self.reg, self.C)\n",
    "            blr.fit(X,y[:,i])\n",
    "            self.classifiers_.append(blr)\n",
    "            \n",
    "        # saving weights\n",
    "        self.w_ = np.hstack([x.w_ for x in self.classifiers_]).T\n",
    "    \n",
    "    def predict_proba(self,X):\n",
    "        \n",
    "        probs = []\n",
    "        for blr in self.classifiers_:\n",
    "            probs.append(blr.predict_proba(X).reshape((len(X),1))) # get probability for each classifier\n",
    "    \n",
    "        return np.hstack(probs) # make into single matrix\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \n",
    "        return np.argmax(self.predict_proba(X),axis=1) # take argmax along row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Testing Dataset:  0.1505281690140845  -  NoDiabetes\n",
      "Accuracy of Testing Dataset:  0.9242957746478874  -  PreDiabetes\n",
      "Accuracy of Testing Dataset:  0.7931338028169014  -  Diabetes\n"
     ]
    }
   ],
   "source": [
    "# Validating training dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression(optimization='newton',eta=0.9, regularization='none', iterations=10, C=0)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating training dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "lr = LogisticRegression_MSE(optimization='newton',eta=0.9, regularization='none', iterations=10, C=0)\n",
    "for i,col in zip(range(len(y_test[0,:])), targets):\n",
    "    lr.fit(X_train, y_train)\n",
    "    yhat = lr.predict(X_test)\n",
    "    print(\"Accuracy of Testing Dataset: \", accuracy_score(y_test[:,i],yhat), \" - \", col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we attempt to implement mean square error (MSE) to optimize logistic regression. We use scikit's brier score function so that we can calculate a mean squared difference between the predicted probability and the actual outcome. It is clear that there is a noticeable drop in the accuracy of 2 of the 3 target features, therefore indicating that MSE is not better than binary cross entropy when optimizing a logistic regression operator. This is likely due to the fact that the MSE loss function is not convex. As a result, you are more likely to get stuck at a local minimum instead of reaching the global minimum, which drastically reduces the accuracy of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
