{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13676a2e",
   "metadata": {},
   "source": [
    "# Lab 3: Extending Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db54406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7138423c",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea67566d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Diabetes_012  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0           0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
      "1           0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
      "2           0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
      "3           0.0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
      "4           0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  ...  AnyHealthcare  \\\n",
      "0                   0.0           0.0     0.0  ...            1.0   \n",
      "1                   0.0           1.0     0.0  ...            0.0   \n",
      "2                   0.0           0.0     1.0  ...            1.0   \n",
      "3                   0.0           1.0     1.0  ...            1.0   \n",
      "4                   0.0           1.0     1.0  ...            1.0   \n",
      "\n",
      "   NoDocbcCost  GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  \\\n",
      "0          0.0      5.0      18.0      15.0       1.0  0.0   9.0        4.0   \n",
      "1          1.0      3.0       0.0       0.0       0.0  0.0   7.0        6.0   \n",
      "2          1.0      5.0      30.0      30.0       1.0  0.0   9.0        4.0   \n",
      "3          0.0      2.0       0.0       0.0       0.0  0.0  11.0        3.0   \n",
      "4          0.0      2.0       3.0       0.0       0.0  0.0  11.0        5.0   \n",
      "\n",
      "   Income  \n",
      "0     3.0  \n",
      "1     1.0  \n",
      "2     8.0  \n",
      "3     6.0  \n",
      "4     4.0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "Index(['Diabetes_012', 'HighBP', 'HighChol', 'CholCheck', 'BMI', 'Smoker',\n",
      "       'Stroke', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies',\n",
      "       'HvyAlcoholConsump', 'AnyHealthcare', 'NoDocbcCost', 'GenHlth',\n",
      "       'MentHlth', 'PhysHlth', 'DiffWalk', 'Sex', 'Age', 'Education',\n",
      "       'Income'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Diabetes_Dataset/diabetes_012_health_indicators_BRFSS2015.csv')\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27e37c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        NoDiabetes  PreDiabetes  Diabetes  HighBP  HighChol  CholCheck   BMI  \\\n",
      "0              1.0          0.0       0.0     1.0       1.0        1.0  40.0   \n",
      "1              1.0          0.0       0.0     0.0       0.0        0.0  25.0   \n",
      "2              1.0          0.0       0.0     1.0       1.0        1.0  28.0   \n",
      "3              1.0          0.0       0.0     1.0       0.0        1.0  27.0   \n",
      "4              1.0          0.0       0.0     1.0       1.0        1.0  24.0   \n",
      "...            ...          ...       ...     ...       ...        ...   ...   \n",
      "253675         1.0          0.0       0.0     1.0       1.0        1.0  45.0   \n",
      "253676         0.0          0.0       1.0     1.0       1.0        1.0  18.0   \n",
      "253677         1.0          0.0       0.0     0.0       0.0        1.0  28.0   \n",
      "253678         1.0          0.0       0.0     1.0       0.0        1.0  23.0   \n",
      "253679         0.0          0.0       1.0     1.0       1.0        1.0  25.0   \n",
      "\n",
      "        Smoker  Stroke  HeartDiseaseorAttack  ...  AnyHealthcare  NoDocbcCost  \\\n",
      "0          1.0     0.0                   0.0  ...            1.0          0.0   \n",
      "1          1.0     0.0                   0.0  ...            0.0          1.0   \n",
      "2          0.0     0.0                   0.0  ...            1.0          1.0   \n",
      "3          0.0     0.0                   0.0  ...            1.0          0.0   \n",
      "4          0.0     0.0                   0.0  ...            1.0          0.0   \n",
      "...        ...     ...                   ...  ...            ...          ...   \n",
      "253675     0.0     0.0                   0.0  ...            1.0          0.0   \n",
      "253676     0.0     0.0                   0.0  ...            1.0          0.0   \n",
      "253677     0.0     0.0                   0.0  ...            1.0          0.0   \n",
      "253678     0.0     0.0                   0.0  ...            1.0          0.0   \n",
      "253679     0.0     0.0                   1.0  ...            1.0          0.0   \n",
      "\n",
      "        GenHlth  MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  Income  \n",
      "0           5.0      18.0      15.0       1.0  0.0   9.0        4.0     3.0  \n",
      "1           3.0       0.0       0.0       0.0  0.0   7.0        6.0     1.0  \n",
      "2           5.0      30.0      30.0       1.0  0.0   9.0        4.0     8.0  \n",
      "3           2.0       0.0       0.0       0.0  0.0  11.0        3.0     6.0  \n",
      "4           2.0       3.0       0.0       0.0  0.0  11.0        5.0     4.0  \n",
      "...         ...       ...       ...       ...  ...   ...        ...     ...  \n",
      "253675      3.0       0.0       5.0       0.0  1.0   5.0        6.0     7.0  \n",
      "253676      4.0       0.0       0.0       1.0  0.0  11.0        2.0     4.0  \n",
      "253677      1.0       0.0       0.0       0.0  0.0   2.0        5.0     2.0  \n",
      "253678      3.0       0.0       0.0       0.0  1.0   7.0        5.0     1.0  \n",
      "253679      2.0       0.0       0.0       0.0  0.0   9.0        6.0     2.0  \n",
      "\n",
      "[253680 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating target features by modifying first column in original dataframe such that we have 3 features consisting of binary values for\n",
    "# no diabetes, prediabetes, and diabetes where 0 is False and 1 is True\n",
    "target_array = np.zeros((len(df),4))\n",
    "for i in range(len(df)):\n",
    "    target_array[i,0] = df['Diabetes_012'].values[i]\n",
    "for i in range(len(target_array)):\n",
    "    # no diabetes\n",
    "    if target_array[i,0] == 0:\n",
    "        target_array[i,1] = 1\n",
    "    # prediabetes\n",
    "    if target_array[i,0] == 1:\n",
    "        target_array[i,2] = 1\n",
    "    # diabetes\n",
    "    if target_array[i,0] == 2:\n",
    "        target_array[i,3] = 1\n",
    "\n",
    "# Adding new target columns to original dataframe\n",
    "target_columns = ['NoDiabetes', 'PreDiabetes', 'Diabetes']\n",
    "for i in range(target_array.shape[1]-1):\n",
    "    df.insert(i, target_columns[i], target_array[:,1:][:,i], True)\n",
    "df_target = df.drop('Diabetes_012', axis=1)\n",
    "print(df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "247a7ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to remove the target column of diabetes in original dataframe\n",
    "columns = list(df_target.columns)\n",
    "targets = ['NoDiabetes', 'PreDiabetes', 'Diabetes']\n",
    "for col in targets:\n",
    "    columns.remove(col)\n",
    "    \n",
    "# Splitting dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=.20, random_state=42, shuffle=True)\n",
    "\n",
    "X_test  = test[columns].to_numpy()\n",
    "X_train = train[columns].to_numpy()\n",
    "y_test  = {}\n",
    "y_train = {}\n",
    "\n",
    "for col in targets:\n",
    "    y_test[col]  = test[col].to_numpy()\n",
    "    y_train[col] = train[col].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383047c3",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ee3a0",
   "metadata": {},
   "source": [
    "### Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2dcf2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Binary Logistic Regression Object, Not Trainable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20, optimization='sd', regularization='none', C=0.001):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.opt = optimization\n",
    "        self.reg = regularization\n",
    "        self.C = C\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Base Binary Logistic Regression Object, Not Trainable'\n",
    "    \n",
    "    # convenience, private and static:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(X):\n",
    "        return np.hstack((np.ones((X.shape[0],1)),X)) # add bias term\n",
    "    \n",
    "    # public:\n",
    "    def predict_proba(self, X, _add_bias=True):\n",
    "        # add bias term if requested\n",
    "        Xb = self._add_bias(X) if _add_bias else X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,X):\n",
    "        return (self.predict_proba(X)>0.5) # return the actual prediction\n",
    "        \n",
    "blr = BinaryLogisticRegressionBase(0.1)\n",
    "print(blr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef483fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from numpy.linalg import pinv\n",
    "\n",
    "# inherit from base class\n",
    "class BinaryLogisticRegression(BinaryLogisticRegressionBase):\n",
    "    # private:\n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "        \n",
    "    # creating ability to choose optimization technique\n",
    "    def _get_gradient(self,X,y):\n",
    "        \n",
    "        gradient = None\n",
    "        if self.opt == 'sd': gradient = self.steepest_descent\n",
    "        elif self.opt == 'sgd': gradient = self.stochastic_gradient_descent\n",
    "        elif self.opt == 'newton': gradient = self.newton\n",
    "        return gradient(X,y)\n",
    "    \n",
    "    def steepest_descent(self,X,y):\n",
    "    \n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def stochastic_gradient_descent(self,X,y):\n",
    "        \n",
    "        idx = int(np.random.rand()*len(y)) # grab random instance\n",
    "        ydiff = y[idx]-self.predict_proba(X[idx],add_bias=False) # get y difference (now scalar)\n",
    "        gradient = X[idx] * ydiff[:,np.newaxis] # make ydiff a column vector and multiply through\n",
    "        \n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return gradient\n",
    "    \n",
    "    def newton(self,X,y):\n",
    "        \n",
    "        g = self.predict_proba(X,add_bias=False).ravel() # get sigmoid value for all classes\n",
    "        hessian = X.T @ np.diag(g*(1-g)) @ X - 2 * self.C # calculate the hessian\n",
    "\n",
    "        ydiff = y-g # get y difference\n",
    "        gradient = np.sum(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        gradient[1:] += -2 * self.w_[1:] * self.C\n",
    "        \n",
    "        return pinv(hessian) @ gradient\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        # increase stability, redefine sigmoid operation\n",
    "        return expit(theta) #1/(1+np.exp(-theta))\n",
    "    \n",
    "    # regularization methods\n",
    "    def _get_reg_gradient(self):\n",
    "        \n",
    "        if self.reg == 'none':\n",
    "            return self.w_[1:]\n",
    "        elif self.reg == 'L1':\n",
    "            return np.sign(self.w_[1:])\n",
    "        elif self.reg == 'L2':\n",
    "            return -2 * self.w_[1:]\n",
    "        elif self.reg == 'L1_L2':\n",
    "            return -2 * self.w_[1:] + np.sign(self.w_[1:])    \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        Xb = self._add_bias(X) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((num_features,1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3499133",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b9b96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
