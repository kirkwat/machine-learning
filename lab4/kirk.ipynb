{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Four: The Multi-Layer Perceptron\n",
    "\n",
    "Team: Miro Ronac, Kirk Watson, Brandon Vincitore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load, Split, and Balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 74001\n",
      "Size after removing missing data: 72718\n",
      "Columns to encode as integers:  State County\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>TotalPop</th>\n",
       "      <th>Men</th>\n",
       "      <th>Women</th>\n",
       "      <th>Hispanic</th>\n",
       "      <th>White</th>\n",
       "      <th>Black</th>\n",
       "      <th>Native</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Pacific</th>\n",
       "      <th>...</th>\n",
       "      <th>Walk</th>\n",
       "      <th>OtherTransp</th>\n",
       "      <th>WorkAtHome</th>\n",
       "      <th>MeanCommute</th>\n",
       "      <th>Employed</th>\n",
       "      <th>PrivateWork</th>\n",
       "      <th>PublicWork</th>\n",
       "      <th>SelfEmployed</th>\n",
       "      <th>FamilyWork</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1845</td>\n",
       "      <td>899</td>\n",
       "      <td>946</td>\n",
       "      <td>2.4</td>\n",
       "      <td>86.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>881</td>\n",
       "      <td>74.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2172</td>\n",
       "      <td>1167</td>\n",
       "      <td>1005</td>\n",
       "      <td>1.1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>54.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.2</td>\n",
       "      <td>852</td>\n",
       "      <td>75.9</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3385</td>\n",
       "      <td>1533</td>\n",
       "      <td>1852</td>\n",
       "      <td>8.0</td>\n",
       "      <td>61.4</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>23.1</td>\n",
       "      <td>1482</td>\n",
       "      <td>73.3</td>\n",
       "      <td>21.1</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4267</td>\n",
       "      <td>2001</td>\n",
       "      <td>2266</td>\n",
       "      <td>9.6</td>\n",
       "      <td>80.3</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>25.9</td>\n",
       "      <td>1849</td>\n",
       "      <td>75.8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>9965</td>\n",
       "      <td>5054</td>\n",
       "      <td>4911</td>\n",
       "      <td>0.9</td>\n",
       "      <td>77.5</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4787</td>\n",
       "      <td>71.4</td>\n",
       "      <td>24.1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   State  TotalPop   Men  Women  Hispanic  White  Black  Native  Asian  \\\n",
       "0      0      1845   899    946       2.4   86.3    5.2     0.0    1.2   \n",
       "1      0      2172  1167   1005       1.1   41.6   54.5     0.0    1.0   \n",
       "2      0      3385  1533   1852       8.0   61.4   26.5     0.6    0.7   \n",
       "3      0      4267  2001   2266       9.6   80.3    7.1     0.5    0.2   \n",
       "4      0      9965  5054   4911       0.9   77.5   16.4     0.0    3.1   \n",
       "\n",
       "   Pacific  ...  Walk  OtherTransp  WorkAtHome  MeanCommute  Employed  \\\n",
       "0      0.0  ...   0.5          0.0         2.1         24.5       881   \n",
       "1      0.0  ...   0.0          0.5         0.0         22.2       852   \n",
       "2      0.4  ...   1.0          0.8         1.5         23.1      1482   \n",
       "3      0.0  ...   1.5          2.9         2.1         25.9      1849   \n",
       "4      0.0  ...   0.8          0.3         0.7         21.0      4787   \n",
       "\n",
       "   PrivateWork  PublicWork  SelfEmployed  FamilyWork  Unemployment  \n",
       "0         74.2        21.2           4.5         0.0           4.6  \n",
       "1         75.9        15.0           9.0         0.0           3.4  \n",
       "2         73.3        21.1           4.8         0.7           4.7  \n",
       "3         75.8        19.7           4.5         0.0           6.1  \n",
       "4         71.4        24.1           4.5         0.0           2.3  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Load the data into memory and save it to a pandas data frame\n",
    "df = pd.read_csv('acs2017_census_tract_data.csv')\n",
    "print('Dataset Size:', df.shape[0])\n",
    "#Remove any observations that having missing data\n",
    "df.dropna(inplace=True)\n",
    "print('Size after removing missing data:', df.shape[0])\n",
    "#Encode any string data as integers\n",
    "print('Columns to encode as integers: ', df.select_dtypes(include='object').columns[0],df.select_dtypes(include='object').columns[1])\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    encoder = LabelEncoder()\n",
    "    df[col] = encoder.fit_transform(df[col])\n",
    "    \n",
    "#Remove non-continuous numeric feature data\n",
    "continuous_features = list(df.columns)\n",
    "for col in set([\"ChildPoverty\", \"TractId\", \"State\", \"County\"]):\n",
    "    continuous_features.remove(col)\n",
    "    \n",
    "#Set y as child poverty rate and remove unneeded columns\n",
    "y = df.ChildPoverty\n",
    "X = df.drop([\"ChildPoverty\", \"TractId\", \"County\"], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You have the option of keeping the \"county\" variable or removing it. Be sure to discuss why you decided to keep/remove this variable.**\n",
    "\n",
    "TODO - Explain why we decided to remove the county variable and other variables\n",
    "\n",
    "(ChildPoverty b/c thats our prediction task, TractId b/c its not useful, and\n",
    "County b/c after running the nn with county still in the data, the nn couldnt converge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: (58174, 34)\n",
      "Testing Set Size: (14544, 34)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Split the dataset into 80% for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.to_numpy(), y.to_numpy(),\n",
    "                                                    test_size=.20, random_state=42)\n",
    "print(\"Training Set Size:\",X_train.shape)\n",
    "print(\"Testing Set Size:\",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin Quartile Ranges: [  0.    6.2  16.4  31.7 100. ]\n"
     ]
    }
   ],
   "source": [
    "# Balance the dataset so that about the same number of instances are within each class\n",
    "y_train, bins = pd.qcut(y_train, q=4, labels=[0,1,2,3], retbins=True) # constructing 4 classification levels\n",
    "print(\"Bin Quartile Ranges:\",bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We balanced the dataset above by identifying 4 quartile ranges using a quantile-based discretization function from Pandas. This function is able to create equal-sized buckets of observations based on the \"ChildPoverty\" feature. This method utilized to balance the dataset creates an accurate representation of the entire dataset which is necessary when training our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bin Class Sizes: [14596 14580 14476 14522]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we can see that we did a good job of balancing our quantization for the training set.\n"
     ]
    }
   ],
   "source": [
    "# Output number of instances and plot\n",
    "print(\"Training Bin Class Sizes:\",np.bincount(y_train))\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.pie(np.bincount(y_train),\n",
    "        labels=['Very low', 'Moderate', 'High', 'Very high'],\n",
    "        autopct='%.2f%%')\n",
    "plt.show()\n",
    "print('Here we can see that we did a good job of balancing our quantization for the training set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Bin Class Sizes: [3633 3753 3599 3559]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here we can see that our test set is also balanced relatively well.\n"
     ]
    }
   ],
   "source": [
    "y_test = pd.cut(y_test, bins, labels=[0,1,2,3], include_lowest=True)\n",
    "print(\"Test Bin Class Sizes:\",np.bincount(y_test))\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.pie(np.bincount(y_test),\n",
    "        labels=['Very low', 'Moderate', 'High', 'Very high'],\n",
    "        autopct='%.2f%%')\n",
    "plt.show()\n",
    "print('Here we can see that our test set is also balanced relatively well.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases, the testing set should not be balanced while the training set should be balanced. The training set should be balanced to avoid creating biases to over-represented or under-represented classes when training the classifier. Not balancing the testing set allows for a true performance evaluation of the classifier. Testing sets represent untampered data that the classifier might experience in the real world. A balanced testing set is unrealistic and can introduce bias. We will have to work with a balanced test dataset because we need to be able to classify the test dataset within the same child poverty rate ranges as our training set (i.e., the bounds we set on each poverty rate needs to remain the same throughout our model to avoid misclassification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing\n",
    "### Starting with simple base classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "#A simple base classifier, which can't be fit or predicted\n",
    "#Only has internal classes to be used by classes that will subclass it\n",
    "class TwoLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0,size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        \n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        return W1, W2\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new\n",
    "    \n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W1, W2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        # only compute for non-bias terms\n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "    \n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = np.mean((Y_enc-A3)**2)\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _feedforward(self, X, W1, W2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        A1 = self._add_bias_unit(X, how='column')\n",
    "        A1 = A1.T\n",
    "        Z1 = W1 @ A1\n",
    "        A2 = self._sigmoid(Z1)\n",
    "        A2 = self._add_bias_unit(A2, how='row')\n",
    "        Z2 = W2 @ A2\n",
    "        A3 = self._sigmoid(Z2)\n",
    "        return A1, Z1, A2, Z2, A3\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = -2*(Y_enc-A3)*A3*(1-A3)  # last layer sensitivity\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2) # back prop the sensitivity \n",
    "        \n",
    "        grad2 = V2 @ A2.T # no bias on final layer\n",
    "        grad1 = V1[1:,:] @ A1.T # dont back prop sensitivity of bias\n",
    "        \n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        _, _, _, _, A3 = self._feedforward(X, self.W1, self.W2)\n",
    "        y_pred = np.argmax(A3, axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add mini-batch to vectorized TLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from https://github.com/eclarson/MachineLearningNotebooks/blob/master/08.%20Practical_NeuralNets.ipynb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class TLPMiniBatch(TwoLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W1, self.W2 = self._initialize_weights()\n",
    "\n",
    "        rho_W1_prev = np.zeros(self.W1.shape)\n",
    "        rho_W2_prev = np.zeros(self.W2.shape)\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # \\frac{\\eta}{1+\\epsilon\\cdot k}\n",
    "            eta = self.eta / (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A1, Z1, A2, Z2, A3 = self._feedforward(X_data[idx],\n",
    "                                                       self.W1,\n",
    "                                                       self.W2)\n",
    "                \n",
    "                cost = self._cost(A3,Y_enc[:, idx],self.W1,self.W2)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(A1=A1, A2=A2, A3=A3, Z1=Z1, Z2=Z2, \n",
    "                                                  Y_enc=Y_enc[:, idx],\n",
    "                                                  W1=self.W1,W2=self.W2)\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W1, rho_W2 = eta * grad1, eta * grad2\n",
    "                self.W1 -= (rho_W1 + (self.alpha * rho_W1_prev))\n",
    "                self.W2 -= (rho_W2 + (self.alpha * rho_W2_prev))\n",
    "                rho_W1_prev, rho_W2_prev = rho_W1, rho_W2\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now implement cross entropy by updating cost function and change V2 in gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from https://github.com/eclarson/MachineLearningNotebooks/blob/master/08.%20Practical_NeuralNets.ipynb\n",
    "class TLPMiniBatchCrossEntropy(TLPMiniBatch):\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = (A3-Y_enc) # <- this is only line that changed\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last step is proper utilization of Glorot initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from https://github.com/eclarson/MachineLearningNotebooks/blob/master/08.%20Practical_NeuralNets.ipynb\n",
    "class TLPBetterInitial(TLPMiniBatchCrossEntropy):             \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights Glorot and He normalization.\"\"\"\n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden + self.n_features_ + 1))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden, self.n_features_ + 1))\n",
    "        W1[:,:1] = 0\n",
    "        \n",
    "        # reduce the final layer magnitude in order to balance the size of the gradients\n",
    "        # between \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden + 1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden + 1)) \n",
    "        W2[:,:1] = 0\n",
    "        \n",
    "        return W1, W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def print_result(nn,X_train,y_train,X_test,y_test,title=\"\",color=\"red\"):\n",
    "    \n",
    "    print(\"=================\")\n",
    "    print(title,\":\")\n",
    "    yhat = nn.predict(X_train)\n",
    "    print('Resubstitution acc:',accuracy_score(y_train,yhat))\n",
    "    \n",
    "    yhat = nn.predict(X_test)\n",
    "    print('Validation acc:',accuracy_score(y_test,yhat))\n",
    "    \n",
    "    if hasattr(nn,'val_score_'):\n",
    "        plt.plot(range(len(nn.val_score_)), nn.val_score_, color=color,label=title)\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "    else:\n",
    "        plt.plot(range(len(nn.score_)), nn.score_, color=color,label=title)\n",
    "        plt.ylabel('Resub Accuracy')\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 50/50"
     ]
    }
   ],
   "source": [
    "vals = { 'n_hidden':30, \n",
    "         'C':0.1, 'epochs':50, 'eta':0.001, \n",
    "         'alpha':0.001, 'decrease_const':1e-5, 'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "#quantify performance using accuracy\n",
    "nn_better = TLPBetterInitial(**vals)\n",
    "\n",
    "%time nn_better.fit(X_train, y_train, print_progress=1, XY_test=(X_test, y_test))\n",
    "\n",
    "print_result(nn_better,X_train,y_train,X_test,y_test,title=\"Not normalized or one-hot encoded data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize continuous numeric feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "#normalize continuous numeric feature data\n",
    "X_normal = X.copy()\n",
    "X_normal[continuous_features] = standard_scaler.fit_transform(X_normal[continuous_features])\n",
    "#get training and testing set\n",
    "X_train_normal, X_test_normal = train_test_split(X_normal.to_numpy(),\n",
    "                                                    test_size=.20,\n",
    "                                                    random_state=42)\n",
    "#quantify performance using accuracy\n",
    "nn_better = TLPBetterInitial(**vals)\n",
    "\n",
    "%time nn_better.fit(X_train_normal, y_train, print_progress=1, XY_test=(X_test_normal, y_test))\n",
    "\n",
    "print_result(nn_better,X_train_normal,y_train,X_test_normal,y_test,title=\"Normalized continuous data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize continuous numeric feature data AND one hot encode categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get normalized continuous data\n",
    "X_normal_encoded = X_normal.copy()\n",
    "#one-hot encode categorical data aka \"State\" column\n",
    "X_normal_encoded = pd.concat([X_normal_encoded, pd.get_dummies(X_normal_encoded[\"State\"], prefix=\"State\")],axis=1)\n",
    "X_normal_encoded.drop([\"State\"], axis=1, inplace=True)\n",
    "#get training and testing set\n",
    "X_train_norm_enc, X_test_norm_enc = train_test_split(X_normal_encoded.to_numpy(),\n",
    "                                                    test_size=.20,\n",
    "                                                    random_state=42)\n",
    "#quantify performance using accuracy\n",
    "nn_better = TLPBetterInitial(**vals)\n",
    "\n",
    "%time nn_better.fit(X_train_norm_enc, y_train, print_progress=1, XY_test=(X_test_norm_enc, y_test))\n",
    "\n",
    "print_result(nn_better,X_train_norm_enc,y_train,X_test_norm_enc,y_test,title=\"Normalized and one-hot encoded data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare performance of three models\n",
    "\n",
    "Compared to the other models, the model using the unedited dataset had the lowest performance. With 4 identification classes and accuracy of about 25%, this model performed similar to how a random class picker would perform. The low performance can be attributed to the raw dataset which has many features with wide ranges of variabilities. As a result, the model struggled to learn and successfully identify classes.\n",
    "\n",
    "The second model utilized a dataset that normalized continuous numeric features. This modification of the dataset allowed the model to have an increased performance of 60% accuracy, because normalizing the data removed the high variability in each of the continuous numeric features.\n",
    "\n",
    "The third model had the best performance with an accuracy of 73%. In addition to using normalized continuous data, this dataset had categorical data one-hot encoded. The \"State\" feature was the only categorical feature, but it had a wide range of values to account for all 50 states. One-hot encoding the states created binary features for each state which reduced bias to the original \"State\" categorical feature. After evaluating these models, normalizing continuous numeric features and one-hot encoding categorical features is necessary to have optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling\n",
    "\n",
    "### Add support for third layer in multi-layer perceptron\n",
    "\n",
    "#### Starting with simple base classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - Add support for a third layer in the multi-layer perceptron\n",
    "\n",
    "TODO - Add support for saving (and plotting after training is completed) the average magnitude of the gradient for each layer, for each epoch (like we did in the flipped module for back propagation)\n",
    "\n",
    "TODO - Quantify the performance of the model and graph the magnitudes for each layer versus the number of epochs\n",
    "\n",
    "TODO - repeat for 4th and 5th layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example adapted from https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch12/ch12.ipynb\n",
    "# Original Author: Sebastian Raschka\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "#A simple base classifier, which can't be fit or predicted\n",
    "#Only has internal classes to be used by classes that will subclass it\n",
    "class MultiLayerPerceptronBase(object):\n",
    "    def __init__(self, n_hidden=30, layers=3,\n",
    "                 C=0.0, epochs=500, eta=0.001, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_hidden = n_hidden\n",
    "        self.layers=layers\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        \n",
    "    @staticmethod\n",
    "    def _encode_labels(y):\n",
    "        \"\"\"Encode labels into one-hot representation\"\"\"\n",
    "        onehot = pd.get_dummies(y).values.T\n",
    "            \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        W=[]\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        #initialize first weights\n",
    "        W1_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "        W1 = np.random.uniform(-1.0, 1.0,size=W1_num_elems)\n",
    "        W1 = W1.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W\n",
    "        W.append(W1)\n",
    "        #initialize middle weights\n",
    "        for i in range(self.layers - 2):\n",
    "            W_num_elems = (self.n_features_ + 1)*self.n_hidden\n",
    "            W_middle = np.random.uniform(-1.0, 1.0,size=W_num_elems)\n",
    "            W_middle = W_middle.reshape(self.n_hidden, self.n_features_ + 1) # reshape to be W CHECK HERE CHECK HERE\n",
    "            W.append(W_middle)\n",
    "        #initialize last weights\n",
    "        W2_num_elems = (self.n_hidden + 1)*self.n_output_\n",
    "        W2 = np.random.uniform(-1.0, 1.0, size=W2_num_elems)\n",
    "        W2 = W2.reshape(self.n_output_, self.n_hidden + 1)\n",
    "        W.append(W2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(z):\n",
    "        \"\"\"Use scipy.special.expit to avoid overflow\"\"\"\n",
    "        # 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias_unit(X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            ones = np.ones((X.shape[0], 1))\n",
    "            X_new = np.hstack((ones, X))\n",
    "        elif how == 'row':\n",
    "            ones = np.ones((1, X.shape[1]))\n",
    "            X_new = np.vstack((ones, X))\n",
    "        return X_new \n",
    "\n",
    "    @staticmethod\n",
    "    def _L2_reg(lambda_, W):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        #only compute for non-bias terms\n",
    "        #from each layer get weight total\n",
    "        weight_total=0\n",
    "        for weight in W:\n",
    "            weight_total += np.mean(weight[:, 1:] ** 2)\n",
    "        return (lambda_/2.0) * np.sqrt(weight_total)\n",
    "        \n",
    "        return (lambda_/2.0) * np.sqrt(np.mean(W1[:, 1:] ** 2) + np.mean(W2[:, 1:] ** 2))\n",
    "\n",
    "    def _cost(self,A,Y_enc,W):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num(Y_enc*np.log(A[len(A) - 1])+(1-Y_enc)*np.log(1-A[len(A) - 1])))\n",
    "        L2_term = self._L2_reg(self.l2_C, W)\n",
    "        return cost + L2_term\n",
    "      \n",
    "    def _feedforward(self, X, W):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \"\"\"\n",
    "        A = []\n",
    "        Z = []\n",
    "        #feed foward first layer\n",
    "        A.append(self._add_bias_unit(X, how='column').T)\n",
    "        #feed forward middle layers\n",
    "        for layer in range(self.layers - 1):\n",
    "            Z.append(W[layer] @ A[layer])\n",
    "            A_middle = self._sigmoid(Z[layer])\n",
    "            A_middle = self._add_bias_unit(A_middle, how='row')\n",
    "            A.append(A_middle)\n",
    "        #feed forward last/output layer\n",
    "        Z.append(W[len(W) - 1] @ A[len(A) - 1])\n",
    "        A.append(self._sigmoid(Z[len(Z) - 1]))\n",
    "        return A, Z\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        A, _ = self._feedforward(X, self.W)\n",
    "        y_pred = np.argmax(A[len(A) - 1], axis=0)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add mini-batch to vectorized MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from https://github.com/eclarson/MachineLearningNotebooks/blob/master/08.%20Practical_NeuralNets.ipynb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class MLPMiniBatch(MultiLayerPerceptronBase):\n",
    "    def __init__(self, alpha=0.0, decrease_const=0.0, shuffle=True, \n",
    "                 minibatches=1, **kwds):        \n",
    "        # need to add to the original initializer \n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # but keep other keywords\n",
    "        super().__init__(**kwds)\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False, XY_test=None):\n",
    "        \"\"\" Learn weights from training data. With mini-batch\"\"\"\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        Y_enc = self._encode_labels(y)\n",
    "        \n",
    "        # init weights and setup matrices\n",
    "        self.n_features_ = X_data.shape[1]\n",
    "        self.n_output_ = Y_enc.shape[0]\n",
    "        self.W = self._initialize_weights()\n",
    "\n",
    "        rho_W_prev = []\n",
    "        for weight in self.W:\n",
    "            rho_W_prev.append(np.zeros(weight.shape))\n",
    "\n",
    "        self.cost_ = []\n",
    "        self.score_ = []\n",
    "        # get starting acc\n",
    "        self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "        # keep track of validation, if given\n",
    "        if XY_test is not None:\n",
    "            X_test = XY_test[0].copy()\n",
    "            y_test = XY_test[1].copy()\n",
    "            self.val_score_ = []\n",
    "            self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "            \n",
    "        #For each epoch store gradiant averages\n",
    "        gradient_averages = []\n",
    "        for _ in range(self.layers):\n",
    "            gradient_averages.append([])\n",
    "            \n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            # \\frac{\\eta}{1+\\epsilon\\cdot k}\n",
    "            eta = self.eta / (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress>0 and (i+1)%print_progress==0:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx_shuffle = np.random.permutation(y_data.shape[0])\n",
    "                X_data, Y_enc, y_data = X_data[idx_shuffle], Y_enc[:, idx_shuffle], y_data[idx_shuffle]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            mini_cost = []\n",
    "            \n",
    "            #For each mini batch, store gradiant averages\n",
    "            gradients = []\n",
    "            for _ in range(self.layers):\n",
    "                gradients.append([])\n",
    "            \n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                A, Z = self._feedforward(X_data[idx], self.W)\n",
    "                \n",
    "                cost = self._cost(A, Y_enc[:, idx], self.W)\n",
    "                mini_cost.append(cost) # this appends cost of mini-batch only\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                gradient = self._get_gradient(A, Z, Y_enc[:, idx], self.W)\n",
    "                \n",
    "                #for each minibatch layer, get average gradient magnitudes\n",
    "                for index in range(len(gradients)):\n",
    "                    gradients[index].append(np.mean(gradient[index]))\n",
    "\n",
    "                # momentum calculations\n",
    "                rho_W = [g * self.eta for g in gradient]\n",
    "                for index in range(len(rho_W)):\n",
    "                    self.W[index] -= (rho_W[index] + (self.alpha * rho_W_prev[index]))\n",
    "                rho_W_prev = rho_W\n",
    "\n",
    "            self.cost_.append(mini_cost)\n",
    "            self.score_.append(accuracy_score(y_data,self.predict(X_data)))\n",
    "            \n",
    "            if XY_test is not None:\n",
    "                self.val_score_.append(accuracy_score(y_test,self.predict(X_test)))\n",
    "                \n",
    "            #get average magnitude of the gradient for each layer\n",
    "            for index in range(len(gradient_averages)):\n",
    "                gradient_averages[index].append(np.mean(gradients[index]))\n",
    "            \n",
    "        return gradient_averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now implement cross entropy by updating cost function and change V2 in gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted example from https://github.com/eclarson/MachineLearningNotebooks/blob/master/08.%20Practical_NeuralNets.ipynb\n",
    "class MLPMiniBatchCrossEntropy(MLPMiniBatch):\n",
    "    def _cost(self,A,Y_enc,W):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num(Y_enc*np.log(A[len(A) - 1])+(1-Y_enc)*np.log(1-A[len(A) - 1])))\n",
    "        L2_term = self._L2_reg(self.l2_C, W)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A, Z, Y_enc, W):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        #gradient and sensitivity are now lists\n",
    "        grads = []\n",
    "        V = []\n",
    "        for _ in range(self.layers):\n",
    "            grads.append(0)\n",
    "            V.append(0)\n",
    "\n",
    "        #vectorized backpropagation\n",
    "        #calc last sensitivity and gradient\n",
    "        V[-1] = A[-1] - Y_enc\n",
    "        grads[-1] = V[-1] @ A[-2].T\n",
    "        grads[-1][:, 1:] += W[-1][:, 1:] * self.l2_C\n",
    "        \n",
    "        #calc remaining sensitivities and gradients in reverse order\n",
    "        for i in reversed(range(self.layers-1)):\n",
    "            V[i] = A[i+1]*(1-A[i+1])*(W[i+1].T @ V[i+1])\n",
    "            grads[i] = V[i][1:,:] @ A[i].T\n",
    "            grads[i][:, 1:] += W[i][:, 1:] * self.l2_C\n",
    "            V[i] = V[i][1:,:]\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from https://github.com/eclarson/MachineLearningNotebooks/blob/master/08.%20Practical_NeuralNets.ipynb\n",
    "class TLPMiniBatchCrossEntropy(TLPMiniBatch):\n",
    "    def _cost(self,A3,Y_enc,W1,W2):\n",
    "        '''Get the objective function value'''\n",
    "        cost = -np.mean(np.nan_to_num((Y_enc*np.log(A3)+(1-Y_enc)*np.log(1-A3))))\n",
    "        L2_term = self._L2_reg(self.l2_C, W1, W2)\n",
    "        return cost + L2_term\n",
    "    \n",
    "    def _get_gradient(self, A1, A2, A3, Z1, Z2, Y_enc, W1, W2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "        \"\"\"\n",
    "        # vectorized backpropagation\n",
    "        V2 = (A3-Y_enc) # <- this is only line that changed\n",
    "        V1 = A2*(1-A2)*(W2.T @ V2)\n",
    "        \n",
    "        grad2 = V2 @ A2.T\n",
    "        grad1 = V1[1:,:] @ A1.T\n",
    "        \n",
    "        # regularize weights that are not bias terms\n",
    "        grad1[:, 1:] += W1[:, 1:] * self.l2_C\n",
    "        grad2[:, 1:] += W2[:, 1:] * self.l2_C\n",
    "\n",
    "        return grad1, grad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBetterInitial(MLPMiniBatchCrossEntropy):\n",
    "    def __init__(self, layers=3, n_hidden=[30], C=0.0, epochs=200, eta=0.001, random_state=None,\n",
    "                alpha=0.0, decrease_const=0.0, shuffle=True, minibatches=1):\n",
    "        np.random.seed(random_state)\n",
    "        self.layers = layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2_C = C\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        W = []\n",
    "        \n",
    "        init_bound = 4*np.sqrt(6. / (self.n_hidden[0] + self.n_features_ + 1))\n",
    "        W1 = np.random.uniform(-init_bound, init_bound,(self.n_hidden[0], self.n_features_ + 1))\n",
    "        W1[:,:1] = 0\n",
    "        W.append(W1)\n",
    "        \n",
    "        for idx in range(self.layers - 2):\n",
    "            init_bound = 4*np.sqrt(6 / (self.n_hidden[idx + 1] + self.n_hidden[idx] + 1))\n",
    "            WTemp = np.random.uniform(-init_bound, init_bound,(self.n_hidden[idx + 1], self.n_hidden[idx] + 1))\n",
    "            WTemp[:,:1] = 0\n",
    "            W.append(WTemp)\n",
    "        \n",
    "        init_bound = 4*np.sqrt(6 / (self.n_output_ + self.n_hidden[self.layers - 2] + 1))\n",
    "        W2 = np.random.uniform(-init_bound, init_bound,(self.n_output_, self.n_hidden[self.layers - 2] + 1))\n",
    "        W2[:,:1] = 0\n",
    "        W.append(W2)\n",
    "        \n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "def print_result(nn,X_train,y_train,X_test,y_test,grads, title=\"\",color=\"red\"):\n",
    "    \n",
    "    print(\"=================\")\n",
    "    print(title,\":\")\n",
    "    yhat = nn.predict(X_train)\n",
    "    print('Resubstitution acc:',accuracy_score(y_train,yhat))\n",
    "    \n",
    "    yhat = nn.predict(X_test)\n",
    "    print('Validation acc:',accuracy_score(y_test,yhat))\n",
    "    \n",
    "    if hasattr(nn,'val_score_'):\n",
    "        plt.plot(range(len(nn.val_score_)), nn.val_score_, color=color,label=title)\n",
    "        plt.ylabel('Validation Accuracy')\n",
    "    else:\n",
    "        plt.plot(range(len(nn.score_)), nn.score_, color=color,label=title)\n",
    "        plt.ylabel('Resub Accuracy')\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    for idx in range(len(grads)):\n",
    "        plt.plot(range(len(grads[0])), grads[idx], label=\"grad_\"+str(idx + 1))\n",
    "    \n",
    "    plt.ylabel('Average Magnitude of the Gradient')\n",
    "        \n",
    "    plt.xlabel('Epochs')\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = { 'layers':3, 'n_hidden':[30, 25], \n",
    "         'C':0.1, 'epochs':25, 'eta':0.001, \n",
    "         'alpha':0.001, 'decrease_const':1e-5, 'minibatches':50,\n",
    "         'shuffle':True,'random_state':1}\n",
    "\n",
    "nn3 = MLPBetterInitial(**vals)\n",
    "\n",
    "%time \n",
    "grads = nn3.fit(X_train_norm_enc, y_train, print_progress=1)\n",
    "\n",
    "print_result(nn3,X_train_norm_enc,y_train, X_test_norm_enc, y_test,grads,title=\"Cross Entropy Loss\",color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exceptional Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
